{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from chemistry import Atom, Bond, Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = pd.read_feather('../data/structures.feather')\n",
    "labelled = pd.read_feather('../data/train.feather')\n",
    "unlabelled = pd.read_feather('../data/test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(data):\n",
    "    data.copy()\n",
    "    \n",
    "    m0 = data.merge(structures, left_on=['molecule_name', 'atom_index_0'], right_on=['molecule_name', 'atom_index'], suffixes=('0', '0'))\n",
    "    m1 = data.merge(structures, left_on=['molecule_name', 'atom_index_1'], right_on=['molecule_name', 'atom_index'], suffixes=('1', '1'))\n",
    "    \n",
    "    l0 = m0[['x', 'y', 'z']]\n",
    "    l1 = m1[['x', 'y', 'z']]\n",
    "    d = l0 - l1\n",
    "    d2 = d * d\n",
    "    dist2 = d2.x + d2.y + d2.z\n",
    "    dist = dist2.apply(np.sqrt)\n",
    "    dist.name = 'distance'\n",
    "    \n",
    "    merged = data.join(dist)\n",
    "    merged['atom_0'] = m0.atom\n",
    "    merged['atom_1'] = m1.atom\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_merged = merge(labelled)\n",
    "unlabelled_merged = merge(unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = min(labelled_merged.distance.min(), unlabelled_merged.distance.min())\n",
    "max_dist = max(labelled_merged.distance.max(), unlabelled_merged.distance.max())\n",
    "\n",
    "min_coeff = labelled_merged.scalar_coupling_constant.min()\n",
    "max_coeff = labelled_merged.scalar_coupling_constant.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = min_coeff\n",
    "scale = max_coeff - min_coeff\n",
    "\n",
    "labelled_merged['norm_distance'] = (labelled_merged.distance - min_dist) / (max_dist - min_dist)\n",
    "unlabelled_merged['norm_distance'] = (unlabelled_merged.distance - min_dist) / (max_dist - min_dist)\n",
    "\n",
    "labelled_merged['norm_scc'] = (labelled_merged.scalar_coupling_constant - min_coeff) / (max_coeff - min_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = labelled.type.unique()\n",
    "atoms = structures.atom.unique()\n",
    "\n",
    "index = 0\n",
    "type_index = {}\n",
    "for t in types:\n",
    "    type_index[t] = index\n",
    "    index += 1\n",
    "\n",
    "atom_index = {}\n",
    "for a in atoms:\n",
    "    atom_index[a] = index\n",
    "    index += 2\n",
    "\n",
    "dist_index = index\n",
    "index += 1\n",
    "\n",
    "columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_labelled(data, count=None, train_frac=0.7):\n",
    "    n_labelled = count if count is not None else len(labelled)\n",
    "    n_train = int(n_labelled * train_frac)\n",
    "    n_test = n_labelled - n_train\n",
    "    indices = np.arange(0, n_labelled)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[0:n_train]\n",
    "    test_indices = indices[n_train:]\n",
    "    \n",
    "    train = data.iloc[train_indices, :]\n",
    "    test = data.iloc[test_indices, :]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def make_input(data, columns, type_index, atom_index, dist_index):\n",
    "    n = len(data)\n",
    "    input = np.zeros((columns, n), dtype='float32')\n",
    "    \n",
    "    for t in type_index:\n",
    "        input[type_index[t], data.type == t] = 1\n",
    "        \n",
    "    for a in atom_index:\n",
    "        input[atom_index[a], data.atom_0 == t] = 1\n",
    "        input[atom_index[a] + 1, data.atom_1 == t] = 1\n",
    "        \n",
    "    input[dist_index] = data.norm_distance\n",
    "        \n",
    "    return input.T\n",
    "\n",
    "def make_output(data):\n",
    "    n = len(data)\n",
    "    output = np.zeros(n, dtype='float32')\n",
    "    output[:] = data.norm_scc\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'molecule_name', 'atom_index_0', 'atom_index_1', 'type',\n",
       "       'scalar_coupling_constant', 'distance', 'atom_0', 'atom_1',\n",
       "       'norm_distance', 'norm_scc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21000, 19), (21000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_train, labelled_test = partition_labelled(labelled_merged, 30000)\n",
    "\n",
    "labelled_train_input = make_input(labelled_train, columns, type_index, atom_index, dist_index)\n",
    "labelled_train_output = make_output(labelled_train)\n",
    "\n",
    "labelled_test_input = make_input(labelled_test, columns, type_index, atom_index, dist_index)\n",
    "labelled_test_output = make_output(labelled_test)\n",
    "\n",
    "labelled_train_input.shape, labelled_train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(Sequence):\n",
    "    def __init__(self, input_data, output_data, batch_size):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.l = len(self.output_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.l // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        j = (idx + 1) * self.batch_size\n",
    "        return self.input_data[i:j, :], self.output_data[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_model():\n",
    "    i = Input(shape=(columns,))\n",
    "    \n",
    "    x = Dense(64, activation='relu')(i)\n",
    "    x = Dense(32, activation='relu')(i)\n",
    "    x = Dense(16, activation='relu')(i)\n",
    "    x = Dense(8, activation='relu')(x)\n",
    "    \n",
    "    o = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=[i], outputs=[o])\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 16)                320       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 465\n",
      "Trainable params: 465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/64\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 0.2555\n",
      "Epoch 2/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.1371\n",
      "Epoch 3/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0971\n",
      "Epoch 4/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0756\n",
      "Epoch 5/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0650\n",
      "Epoch 6/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0542\n",
      "Epoch 7/64\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.0425\n",
      "Epoch 8/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0299\n",
      "Epoch 9/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0206\n",
      "Epoch 10/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0194\n",
      "Epoch 11/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0191\n",
      "Epoch 12/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0190\n",
      "Epoch 13/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0190\n",
      "Epoch 14/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0190\n",
      "Epoch 15/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0189\n",
      "Epoch 16/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0189\n",
      "Epoch 17/64\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.0189\n",
      "Epoch 18/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0189\n",
      "Epoch 19/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0189\n",
      "Epoch 20/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0189\n",
      "Epoch 21/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0189\n",
      "Epoch 22/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0189\n",
      "Epoch 23/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0189\n",
      "Epoch 24/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 25/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 26/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 27/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 28/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 29/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 30/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 31/64\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.0188\n",
      "Epoch 32/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 33/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 34/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 35/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 36/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 37/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 38/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 39/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 40/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 41/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 42/64\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.0188\n",
      "Epoch 43/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 44/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 45/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 46/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 47/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 48/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 49/64\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.0188\n",
      "Epoch 50/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 51/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 52/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 53/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 54/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 55/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 56/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 57/64\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.0188\n",
      "Epoch 58/64\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.0187\n",
      "Epoch 59/64\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.0188\n",
      "Epoch 60/64\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.0188\n",
      "Epoch 61/64\n",
      "20/20 [==============================] - 0s 16ms/step - loss: 0.0188\n",
      "Epoch 62/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0187\n",
      "Epoch 63/64\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.0188\n",
      "Epoch 64/64\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.0188\n"
     ]
    }
   ],
   "source": [
    "nn_model = make_nn_model()\n",
    "history = nn_model.fit_generator(Batch(labelled_train_input, labelled_train_output, 1024),\n",
    "                                 workers=8, epochs=64, steps_per_epoch=128, verbose=1, use_multiprocessing=True)\n",
    "#history = nn_model.fit(x=labelled_train_input, y=labelled_train_output,\n",
    "#                       validation_split=0.1, epochs=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn_model.predict(labelled_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4175660525245215"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_error(data, test_output, ref_output):\n",
    "    types = data.type.unique()\n",
    "    ntypes = len(types)\n",
    "    sum_log_type_errors = 0\n",
    "    for t in types:\n",
    "        type_test_output = test_output[data.type == t]\n",
    "        type_ref_output = ref_output[data.type == t]\n",
    "        nt = len(type_test_output)\n",
    "        type_error = np.abs(type_test_output - type_ref_output).sum() / nt\n",
    "        log_type_error = np.log(type_error)\n",
    "        sum_log_type_errors += log_type_error\n",
    "        \n",
    "    return sum_log_type_errors / ntypes\n",
    "    \n",
    "compute_error(labelled_test, output, labelled_test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_train_input = make_input(labelled_merged, columns, type_index, atom_index, dist_index)\n",
    "labelled_train_output = make_output(labelled_merged)\n",
    "\n",
    "labelled_train_input.shape, labelled_train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_input = make_input(unlabelled_merged, columns, type_index, atom_index, dist_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = make_nn_model()\n",
    "#history = nn_model.fit_generator(Batch(labelled_train_input, labelled_train_output, 1024),\n",
    "#                                 workers=8, epochs=150, steps_per_epoch=128, verbose=1)\n",
    "history = nn_model.fit(x=labelled_train_input, y=labelled_train_output,\n",
    "                       validation_split=0.1, epochs=128, verbose=1)\n",
    "unlabelled_output = nn_model.predict(unlabelled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_output = unlabelled_output * scale + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'id':unlabelled.id, 'scalar_coupling_constant':pd.Series(unlabelled_output, index=unlabelled.index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('../data/pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
