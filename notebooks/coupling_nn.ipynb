{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import  matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from chemistry import Molecule\n",
    "from util import score\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/molecules_enh.pickle', 'rb') as f:\n",
    "    molecules = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautham/apps/anaconda3/envs/kaggle/lib/python3.7/site-packages/pyarrow/pandas_compat.py:752: FutureWarning:\n",
      "\n",
      ".labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structures = pd.read_feather('../data/structures_enh.feather')\n",
    "molecules_df = pd.read_feather('../data/molecules.feather')\n",
    "labelled = pd.read_feather('../data/train.feather')\n",
    "unlabelled = pd.read_feather('../data/test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_moments = pd.read_feather('../data/dipole_moments.feather')\n",
    "magnetic_shielding_tensors = pd.read_feather('../data/magnetic_shielding_tensors.feather')\n",
    "mulliken_charges = pd.read_feather('../data/mulliken_charges.feather')\n",
    "potential_energy = pd.read_feather('../data/potential_energy.feather')\n",
    "scalar_coupling_contributions = pd.read_feather('../data/scalar_coupling_contributions.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_enh = labelled.merge(molecules_df, left_on='molecule_name', right_on='molecule_name')\n",
    "#labelled_enh.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_enh = unlabelled.merge(molecules_df, left_on='molecule_name', right_on='molecule_name')\n",
    "#unlabelled_enh.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4658147, 4658147)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelled), len(labelled_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_args = dict(dipole_moments=dipole_moments,\n",
    "               magnetic_shielding_tensors=magnetic_shielding_tensors,\n",
    "               mulliken_charges=mulliken_charges,\n",
    "               potential_energy=potential_energy,\n",
    "               scalar_coupling_contributions=scalar_coupling_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dc4cf0d557f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.structures.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_enh.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import NNModel, partition_data\n",
    "\n",
    "print('Running')\n",
    "\n",
    "#data_df = labelled[labelled.type == '1JHC'].head(10)\n",
    "data_df = labelled_enh.head(10)\n",
    "train_df, test_df = partition_data(data_df, train_frac=1)\n",
    "train_df = train_df.copy()\n",
    "        \n",
    "model = NNModel(model_args = dict(molecules=molecules,\n",
    "                                  structures=structures),\n",
    "               **nn_args)\n",
    "#model.corr(train_df, train_df)\n",
    "model.setup_data(train_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_enh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labelled_enh.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['molecule_name']].merge(dipole_moments, left_on='molecule_name', right_on='molecule_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    n = 5\n",
    "    \n",
    "    loss = np.array(history.history['loss'], dtype='float32')\n",
    "    val_loss = np.array(history.history['val_loss'], dtype='float32')\n",
    "    \n",
    "    loss_mean = loss.mean()\n",
    "    loss_nstd = n * loss.std()\n",
    "    \n",
    "    val_loss_mean = val_loss.mean()\n",
    "    val_loss_nstd = n * val_loss.std()\n",
    "       \n",
    "#     mn = min(loss_mean - loss_nstd, val_loss_mean - val_loss_nstd)\n",
    "#     mx = min(loss_mean + loss_nstd, val_loss_mean + val_loss_nstd)\n",
    "    mn = loss_mean - loss_nstd\n",
    "    mx = loss_mean + loss_nstd\n",
    "    \n",
    "#     print(loss.min(), loss.max())\n",
    "#     print(val_loss.min(), val_loss.max())\n",
    "#     print(loss_mean, loss_nstd, val_loss_mean, val_loss_nstd)\n",
    "#     print(mn, mx)\n",
    "    \n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(mn, mx)\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_args = dict(dipole_moments=dipole_moments,\n",
    "               magnetic_shielding_tensors=magnetic_shielding_tensors,\n",
    "               mulliken_charges=mulliken_charges,\n",
    "               potential_energy=potential_energy,\n",
    "               scalar_coupling_contributions=scalar_coupling_contributions,\n",
    "               epochs=200,\n",
    "               learning_rate=0.001,\n",
    "               validation_split=0.3\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from models import NNModel\n",
    "from models import partition_data, NNModel\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as tfbe\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# atom_count_ranges = [(1, 10), (11, 15), (16, 20), (21, 25), (26, 30)]\n",
    "# C_count_ranges = [(1, 5), (6, 6), (7, 7), (8, 9)]\n",
    "coupling_types = sorted(labelled_enh.type.unique())[0:1]\n",
    "def test(data, count=5000):\n",
    "    global model\n",
    "    global history\n",
    "    \n",
    "    plt.figure(figsize=(25, 25))\n",
    "    \n",
    "    for i, t in enumerate(coupling_types, 1):    \n",
    "        data_df = data[data.type == t].head(count)\n",
    "\n",
    "        train_df, valid_df, test_df = partition_data(data_df, train_frac=0.7, valid_frac=0)\n",
    "\n",
    "        if len(train_df) < 10 or len(test_df) < 10:\n",
    "            continue\n",
    "\n",
    "        print(f'Training {len(train_df)} samples for {t}')\n",
    "\n",
    "        model = NNModel(dict(molecules=molecules, \n",
    "                             structures=structures),\n",
    "                        **nn_args)\n",
    "        history = model.fit(train_df, train_df)\n",
    "        \n",
    "        print(f'Evaluating {len(test_df)} samples')\n",
    "        output, score = model.evaluate(test_df, test_df)\n",
    "        print(f'{t} score: {score} (trained on {len(train_df)} elements)')\n",
    "\n",
    "\n",
    "        plt.subplot(4, 4, 2 * i - 1)\n",
    "        plot_history(history, t)\n",
    "        \n",
    "        #print(output.shape, test_df.values.shape)\n",
    "        #pprint(list(zip(output[0:20, 0], test_df.scalar_coupling_constant[0:20])))\n",
    "\n",
    "        plt.subplot(4, 4, 2 * i)\n",
    "        plt.plot(test_df.scalar_coupling_constant, output, '*')\n",
    "        mn = min(test_df.scalar_coupling_constant.min(), output.min())\n",
    "        mx = min(test_df.scalar_coupling_constant.max(), output.max())\n",
    "\n",
    "        plt.plot([mn, mx], [mn, mx])\n",
    "        plt.title(t)\n",
    "                        \n",
    "    plt.show()\n",
    "        \n",
    "test(labelled_enh, 30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = model.model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = w.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.min(), w2.max(), w2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.numeric_input_df.columns[np.abs(w.sum(axis=1)) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 709416 samples for 1JHC\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 496591 samples, validate on 212825 samples\n",
      "Epoch 1/200\n",
      "496591/496591 [==============================] - 42s 84us/sample - loss: 0.4648 - val_loss: 0.3538\n",
      "Epoch 2/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3893 - val_loss: 0.2604\n",
      "Epoch 3/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3784 - val_loss: 0.2468\n",
      "Epoch 4/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3750 - val_loss: 0.2494\n",
      "Epoch 5/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3710 - val_loss: 0.2486\n",
      "Epoch 6/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3815 - val_loss: 0.2548\n",
      "Epoch 7/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3666 - val_loss: 0.2249\n",
      "Epoch 8/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3544 - val_loss: 0.2801\n",
      "Epoch 9/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3461 - val_loss: 0.2666\n",
      "Epoch 10/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3426 - val_loss: 0.2303\n",
      "Epoch 11/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3390 - val_loss: 0.2374\n",
      "Epoch 12/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3383 - val_loss: 0.2164\n",
      "Epoch 13/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3303 - val_loss: 0.2188\n",
      "Epoch 14/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3247 - val_loss: 0.2259\n",
      "Epoch 15/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3214 - val_loss: 0.2273\n",
      "Epoch 16/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3201 - val_loss: 0.2325\n",
      "Epoch 17/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3173 - val_loss: 0.2295\n",
      "Epoch 18/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3180 - val_loss: 0.2258\n",
      "Epoch 19/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3154 - val_loss: 0.2249\n",
      "Epoch 20/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3174 - val_loss: 0.2081\n",
      "Epoch 21/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3124 - val_loss: 0.2294\n",
      "Epoch 22/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3119 - val_loss: 0.2300\n",
      "Epoch 23/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.3107 - val_loss: 0.2299\n",
      "Epoch 24/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3090 - val_loss: 0.2228\n",
      "Epoch 25/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3093 - val_loss: 0.2274\n",
      "Epoch 26/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3078 - val_loss: 0.2329\n",
      "Epoch 27/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3069 - val_loss: 0.2149\n",
      "Epoch 28/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3086 - val_loss: 0.2223\n",
      "Epoch 29/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3056 - val_loss: 0.2234\n",
      "Epoch 30/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3049 - val_loss: 0.2392\n",
      "Epoch 31/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3054 - val_loss: 0.2229\n",
      "Epoch 32/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3049 - val_loss: 0.2231\n",
      "Epoch 33/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3031 - val_loss: 0.2140\n",
      "Epoch 34/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3035 - val_loss: 0.2207\n",
      "Epoch 35/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3044 - val_loss: 0.2275\n",
      "Epoch 36/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3022 - val_loss: 0.2279\n",
      "Epoch 37/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3029 - val_loss: 0.2313\n",
      "Epoch 38/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3031 - val_loss: 0.2282\n",
      "Epoch 39/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3014 - val_loss: 0.2334\n",
      "Epoch 40/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3002 - val_loss: 0.2514\n",
      "Epoch 41/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2994 - val_loss: 0.2281\n",
      "Epoch 42/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2995 - val_loss: 0.2405\n",
      "Epoch 43/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2994 - val_loss: 0.2255\n",
      "Epoch 44/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.3010 - val_loss: 0.2361\n",
      "Epoch 45/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2989 - val_loss: 0.2500\n",
      "Epoch 46/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2991 - val_loss: 0.2313\n",
      "Epoch 47/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2984 - val_loss: 0.2519\n",
      "Epoch 48/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2991 - val_loss: 0.2404\n",
      "Epoch 49/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2995 - val_loss: 0.2428\n",
      "Epoch 50/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2994 - val_loss: 0.2452\n",
      "Epoch 51/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3016 - val_loss: 0.2690\n",
      "Epoch 52/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.3011 - val_loss: 0.2349\n",
      "Epoch 53/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2986 - val_loss: 0.2595\n",
      "Epoch 54/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2978 - val_loss: 0.2454\n",
      "Epoch 55/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2967 - val_loss: 0.2526\n",
      "Epoch 56/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2979 - val_loss: 0.2464\n",
      "Epoch 57/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2962 - val_loss: 0.2557\n",
      "Epoch 58/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2969 - val_loss: 0.2602\n",
      "Epoch 59/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2969 - val_loss: 0.2450\n",
      "Epoch 60/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2963 - val_loss: 0.2463\n",
      "Epoch 61/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2963 - val_loss: 0.2487\n",
      "Epoch 62/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2959 - val_loss: 0.2403\n",
      "Epoch 63/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2962 - val_loss: 0.2550\n",
      "Epoch 64/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2959 - val_loss: 0.2424\n",
      "Epoch 65/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2956 - val_loss: 0.2404\n",
      "Epoch 66/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2960 - val_loss: 0.2444\n",
      "Epoch 67/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2953 - val_loss: 0.2546\n",
      "Epoch 68/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2952 - val_loss: 0.2571\n",
      "Epoch 69/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2941 - val_loss: 0.2482\n",
      "Epoch 70/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2945 - val_loss: 0.2533\n",
      "Epoch 71/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2944 - val_loss: 0.2620\n",
      "Epoch 72/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2947 - val_loss: 0.2590\n",
      "Epoch 73/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2950 - val_loss: 0.2572\n",
      "Epoch 74/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2954 - val_loss: 0.2572\n",
      "Epoch 75/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2946 - val_loss: 0.2419\n",
      "Epoch 76/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2939 - val_loss: 0.2538\n",
      "Epoch 77/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2944 - val_loss: 0.2473\n",
      "Epoch 78/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2944 - val_loss: 0.2521\n",
      "Epoch 79/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2940 - val_loss: 0.2557\n",
      "Epoch 80/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2935 - val_loss: 0.2638\n",
      "Epoch 81/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2931 - val_loss: 0.2669\n",
      "Epoch 82/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2934 - val_loss: 0.2436\n",
      "Epoch 83/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2932 - val_loss: 0.2497\n",
      "Epoch 84/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2936 - val_loss: 0.2593\n",
      "Epoch 85/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2938 - val_loss: 0.2412\n",
      "Epoch 86/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2944 - val_loss: 0.2625\n",
      "Epoch 87/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2947 - val_loss: 0.2531\n",
      "Epoch 88/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2932 - val_loss: 0.2513\n",
      "Epoch 89/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2951 - val_loss: 0.2348\n",
      "Epoch 90/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2932 - val_loss: 0.2405\n",
      "Epoch 91/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2925 - val_loss: 0.2465\n",
      "Epoch 92/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2928 - val_loss: 0.2476\n",
      "Epoch 93/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2920 - val_loss: 0.2610\n",
      "Epoch 94/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2935 - val_loss: 0.2545\n",
      "Epoch 95/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2920 - val_loss: 0.2473\n",
      "Epoch 96/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2923 - val_loss: 0.2563\n",
      "Epoch 97/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2928 - val_loss: 0.2863\n",
      "Epoch 98/200\n",
      "496591/496591 [==============================] - 33s 65us/sample - loss: 0.2926 - val_loss: 0.2592\n",
      "Epoch 99/200\n",
      "496591/496591 [==============================] - 33s 65us/sample - loss: 0.2921 - val_loss: 0.2741\n",
      "Epoch 100/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2921 - val_loss: 0.2716\n",
      "Epoch 101/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2917 - val_loss: 0.2748\n",
      "Epoch 102/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2918 - val_loss: 0.2498\n",
      "Epoch 103/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2914 - val_loss: 0.2543\n",
      "Epoch 104/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2912 - val_loss: 0.2641\n",
      "Epoch 105/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2918 - val_loss: 0.2813\n",
      "Epoch 106/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2911 - val_loss: 0.2662\n",
      "Epoch 107/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2923 - val_loss: 0.2702\n",
      "Epoch 108/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2921 - val_loss: 0.2955\n",
      "Epoch 109/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2923 - val_loss: 0.2670\n",
      "Epoch 110/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2915 - val_loss: 0.2460\n",
      "Epoch 111/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.2917 - val_loss: 0.2582\n",
      "Epoch 112/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2915 - val_loss: 0.2699\n",
      "Epoch 113/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2914 - val_loss: 0.2600\n",
      "Epoch 114/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2914 - val_loss: 0.2453\n",
      "Epoch 115/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2910 - val_loss: 0.2542\n",
      "Epoch 116/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2908 - val_loss: 0.2566\n",
      "Epoch 117/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2916 - val_loss: 0.2747\n",
      "Epoch 118/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2914 - val_loss: 0.2569\n",
      "Epoch 119/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2918 - val_loss: 0.2623\n",
      "Epoch 120/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2922 - val_loss: 0.2611\n",
      "Epoch 121/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2908 - val_loss: 0.2604\n",
      "Epoch 122/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2912 - val_loss: 0.2778\n",
      "Epoch 123/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2918 - val_loss: 0.2796\n",
      "Epoch 124/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2923 - val_loss: 0.2523\n",
      "Epoch 125/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2930 - val_loss: 0.2544\n",
      "Epoch 126/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2921 - val_loss: 0.2548\n",
      "Epoch 127/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2924 - val_loss: 0.2603\n",
      "Epoch 128/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2916 - val_loss: 0.2485\n",
      "Epoch 129/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2905 - val_loss: 0.2572\n",
      "Epoch 130/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2905 - val_loss: 0.2719\n",
      "Epoch 131/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2906 - val_loss: 0.2660\n",
      "Epoch 132/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2910 - val_loss: 0.2803\n",
      "Epoch 133/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2906 - val_loss: 0.2659\n",
      "Epoch 134/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2916 - val_loss: 0.2666\n",
      "Epoch 135/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2912 - val_loss: 0.2637\n",
      "Epoch 136/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2905 - val_loss: 0.2673\n",
      "Epoch 137/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2912 - val_loss: 0.2692\n",
      "Epoch 138/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2903 - val_loss: 0.2612\n",
      "Epoch 139/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2905 - val_loss: 0.2741\n",
      "Epoch 140/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2909 - val_loss: 0.2693\n",
      "Epoch 141/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2905 - val_loss: 0.2652\n",
      "Epoch 142/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2903 - val_loss: 0.2707\n",
      "Epoch 143/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2905 - val_loss: 0.2587\n",
      "Epoch 144/200\n",
      "496591/496591 [==============================] - 32s 64us/sample - loss: 0.2914 - val_loss: 0.2487\n",
      "Epoch 145/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2911 - val_loss: 0.2535\n",
      "Epoch 146/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2901 - val_loss: 0.2772\n",
      "Epoch 147/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2904 - val_loss: 0.2708\n",
      "Epoch 148/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2903 - val_loss: 0.2632\n",
      "Epoch 149/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2913 - val_loss: 0.2456\n",
      "Epoch 150/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2917 - val_loss: 0.2603\n",
      "Epoch 151/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2906 - val_loss: 0.2665\n",
      "Epoch 152/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2897 - val_loss: 0.2653\n",
      "Epoch 153/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2906 - val_loss: 0.2697\n",
      "Epoch 154/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2893 - val_loss: 0.2720\n",
      "Epoch 155/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2901 - val_loss: 0.2483\n",
      "Epoch 156/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2909 - val_loss: 0.2523\n",
      "Epoch 157/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2899 - val_loss: 0.2631\n",
      "Epoch 158/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2896 - val_loss: 0.2588\n",
      "Epoch 159/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2904 - val_loss: 0.2628\n",
      "Epoch 160/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2905 - val_loss: 0.2684\n",
      "Epoch 161/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2901 - val_loss: 0.2621\n",
      "Epoch 162/200\n",
      "496591/496591 [==============================] - 33s 66us/sample - loss: 0.2904 - val_loss: 0.2602\n",
      "Epoch 163/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2901 - val_loss: 0.2335\n",
      "Epoch 164/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2902 - val_loss: 0.2631\n",
      "Epoch 165/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2897 - val_loss: 0.2459\n",
      "Epoch 166/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2896 - val_loss: 0.2508\n",
      "Epoch 167/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2902 - val_loss: 0.2479\n",
      "Epoch 168/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2891 - val_loss: 0.2479\n",
      "Epoch 169/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2888 - val_loss: 0.2562\n",
      "Epoch 170/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2895 - val_loss: 0.2470\n",
      "Epoch 171/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2892 - val_loss: 0.2475\n",
      "Epoch 172/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2890 - val_loss: 0.2628\n",
      "Epoch 173/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2895 - val_loss: 0.2468\n",
      "Epoch 174/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2891 - val_loss: 0.2481\n",
      "Epoch 175/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2889 - val_loss: 0.2435\n",
      "Epoch 176/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2901 - val_loss: 0.2517\n",
      "Epoch 177/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2884 - val_loss: 0.2329\n",
      "Epoch 178/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2882 - val_loss: 0.2631\n",
      "Epoch 179/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2890 - val_loss: 0.2493\n",
      "Epoch 180/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2888 - val_loss: 0.2479\n",
      "Epoch 181/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2882 - val_loss: 0.2387\n",
      "Epoch 182/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2883 - val_loss: 0.2408\n",
      "Epoch 183/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2886 - val_loss: 0.2400\n",
      "Epoch 184/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2889 - val_loss: 0.2257\n",
      "Epoch 185/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2881 - val_loss: 0.2423\n",
      "Epoch 186/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2879 - val_loss: 0.2701\n",
      "Epoch 187/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2878 - val_loss: 0.2677\n",
      "Epoch 188/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2883 - val_loss: 0.2236\n",
      "Epoch 189/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2885 - val_loss: 0.2413\n",
      "Epoch 190/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2884 - val_loss: 0.2483\n",
      "Epoch 191/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2887 - val_loss: 0.2464\n",
      "Epoch 192/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2884 - val_loss: 0.2558\n",
      "Epoch 193/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2884 - val_loss: 0.2465\n",
      "Epoch 194/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2877 - val_loss: 0.2505\n",
      "Epoch 195/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2881 - val_loss: 0.2456\n",
      "Epoch 196/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2879 - val_loss: 0.2536\n",
      "Epoch 197/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2875 - val_loss: 0.2488\n",
      "Epoch 198/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2871 - val_loss: 0.2616\n",
      "Epoch 199/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2869 - val_loss: 0.2423\n",
      "Epoch 200/200\n",
      "496591/496591 [==============================] - 32s 65us/sample - loss: 0.2872 - val_loss: 0.2417\n",
      "Training 43363 samples for 1JHN\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 30354 samples, validate on 13009 samples\n",
      "Epoch 1/200\n",
      "30354/30354 [==============================] - 12s 388us/sample - loss: 0.8920 - val_loss: 0.4602\n",
      "Epoch 2/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.4525 - val_loss: 0.3258\n",
      "Epoch 3/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.3635 - val_loss: 0.2202\n",
      "Epoch 4/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.3345 - val_loss: 0.2062\n",
      "Epoch 5/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.3171 - val_loss: 0.2264\n",
      "Epoch 6/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.3081 - val_loss: 0.2634\n",
      "Epoch 7/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2916 - val_loss: 0.2491\n",
      "Epoch 8/200\n",
      "30354/30354 [==============================] - 2s 67us/sample - loss: 0.3020 - val_loss: 0.2049\n",
      "Epoch 9/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2751 - val_loss: 0.1699\n",
      "Epoch 10/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2777 - val_loss: 0.2238\n",
      "Epoch 11/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2857 - val_loss: 0.2141\n",
      "Epoch 12/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2834 - val_loss: 0.2262\n",
      "Epoch 13/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2674 - val_loss: 0.1853\n",
      "Epoch 14/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2660 - val_loss: 0.2178\n",
      "Epoch 15/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2698 - val_loss: 0.1674\n",
      "Epoch 16/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2655 - val_loss: 0.2126\n",
      "Epoch 17/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2659 - val_loss: 0.2569\n",
      "Epoch 18/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2704 - val_loss: 0.2423\n",
      "Epoch 19/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2753 - val_loss: 0.2743\n",
      "Epoch 20/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2704 - val_loss: 0.2892\n",
      "Epoch 21/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2626 - val_loss: 0.2264\n",
      "Epoch 22/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2647 - val_loss: 0.2509\n",
      "Epoch 23/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2578 - val_loss: 0.2109\n",
      "Epoch 24/200\n",
      "30354/30354 [==============================] - 2s 67us/sample - loss: 0.2632 - val_loss: 0.2250\n",
      "Epoch 25/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2606 - val_loss: 0.1784\n",
      "Epoch 26/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2598 - val_loss: 0.1733\n",
      "Epoch 27/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2696 - val_loss: 0.1792\n",
      "Epoch 28/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2563 - val_loss: 0.1404\n",
      "Epoch 29/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2656 - val_loss: 0.2490\n",
      "Epoch 30/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2582 - val_loss: 0.3328\n",
      "Epoch 31/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2740 - val_loss: 0.1324\n",
      "Epoch 32/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2774 - val_loss: 0.2768\n",
      "Epoch 33/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2627 - val_loss: 0.3005\n",
      "Epoch 34/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2741 - val_loss: 0.3210\n",
      "Epoch 35/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2648 - val_loss: 0.3824\n",
      "Epoch 36/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2700 - val_loss: 0.5202\n",
      "Epoch 37/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2777 - val_loss: 0.2607\n",
      "Epoch 38/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2583 - val_loss: 0.3265\n",
      "Epoch 39/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2607 - val_loss: 0.1775\n",
      "Epoch 40/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2563 - val_loss: 0.3933\n",
      "Epoch 41/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2780 - val_loss: 0.2129\n",
      "Epoch 42/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2596 - val_loss: 0.2102\n",
      "Epoch 43/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2567 - val_loss: 0.2038\n",
      "Epoch 44/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2582 - val_loss: 0.2890\n",
      "Epoch 45/200\n",
      "30354/30354 [==============================] - 2s 66us/sample - loss: 0.2584 - val_loss: 0.1658\n",
      "Epoch 46/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2485 - val_loss: 0.1377\n",
      "Epoch 47/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2513 - val_loss: 0.1605\n",
      "Epoch 48/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2518 - val_loss: 0.1702\n",
      "Epoch 49/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2529 - val_loss: 0.2158\n",
      "Epoch 50/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2508 - val_loss: 0.1462\n",
      "Epoch 51/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2565 - val_loss: 0.2167\n",
      "Epoch 52/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2522 - val_loss: 0.5327\n",
      "Epoch 53/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2480 - val_loss: 0.2303\n",
      "Epoch 54/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2521 - val_loss: 0.2379\n",
      "Epoch 55/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2482 - val_loss: 0.1347\n",
      "Epoch 56/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2536 - val_loss: 0.3361\n",
      "Epoch 57/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2434 - val_loss: 0.1727\n",
      "Epoch 58/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2451 - val_loss: 0.1410\n",
      "Epoch 59/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2442 - val_loss: 0.2070\n",
      "Epoch 60/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2446 - val_loss: 0.2560\n",
      "Epoch 61/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2484 - val_loss: 0.1713\n",
      "Epoch 62/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2493 - val_loss: 0.1289\n",
      "Epoch 63/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2489 - val_loss: 0.4349\n",
      "Epoch 64/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2532 - val_loss: 0.2431\n",
      "Epoch 65/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2453 - val_loss: 0.1477\n",
      "Epoch 66/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2504 - val_loss: 0.2255\n",
      "Epoch 67/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2447 - val_loss: 0.1529\n",
      "Epoch 68/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2518 - val_loss: 2.0484\n",
      "Epoch 69/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2603 - val_loss: 0.7898\n",
      "Epoch 70/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2636 - val_loss: 0.1989\n",
      "Epoch 71/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2606 - val_loss: 0.1570\n",
      "Epoch 72/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2486 - val_loss: 0.1490\n",
      "Epoch 73/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2450 - val_loss: 0.1954\n",
      "Epoch 74/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2506 - val_loss: 0.3288\n",
      "Epoch 75/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2476 - val_loss: 0.2010\n",
      "Epoch 76/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2452 - val_loss: 0.2827\n",
      "Epoch 77/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2485 - val_loss: 0.2344\n",
      "Epoch 78/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2475 - val_loss: 0.1932\n",
      "Epoch 79/200\n",
      "30354/30354 [==============================] - 2s 65us/sample - loss: 0.2424 - val_loss: 0.1828\n",
      "Epoch 80/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2440 - val_loss: 0.1843\n",
      "Epoch 81/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2421 - val_loss: 0.2789\n",
      "Epoch 82/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2439 - val_loss: 0.1905\n",
      "Epoch 83/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2396 - val_loss: 0.1155\n",
      "Epoch 84/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2449 - val_loss: 0.1223\n",
      "Epoch 85/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2382 - val_loss: 0.1245\n",
      "Epoch 86/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2395 - val_loss: 0.1468\n",
      "Epoch 87/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2453 - val_loss: 0.1496\n",
      "Epoch 88/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2441 - val_loss: 0.1442\n",
      "Epoch 89/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2481 - val_loss: 0.1568\n",
      "Epoch 90/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2359 - val_loss: 0.1732\n",
      "Epoch 91/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2418 - val_loss: 0.2860\n",
      "Epoch 92/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2362 - val_loss: 0.1216\n",
      "Epoch 93/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2381 - val_loss: 0.1423\n",
      "Epoch 94/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2346 - val_loss: 0.1350\n",
      "Epoch 95/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2379 - val_loss: 0.1374\n",
      "Epoch 96/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2348 - val_loss: 0.1207\n",
      "Epoch 97/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2327 - val_loss: 0.1267\n",
      "Epoch 98/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2323 - val_loss: 0.1224\n",
      "Epoch 99/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2324 - val_loss: 0.1278\n",
      "Epoch 100/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2321 - val_loss: 0.1396\n",
      "Epoch 101/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2358 - val_loss: 0.1822\n",
      "Epoch 102/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2337 - val_loss: 0.1303\n",
      "Epoch 103/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2334 - val_loss: 0.1457\n",
      "Epoch 104/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2339 - val_loss: 0.1313\n",
      "Epoch 105/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2316 - val_loss: 0.1370\n",
      "Epoch 106/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2322 - val_loss: 0.1273\n",
      "Epoch 107/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2359 - val_loss: 0.1555\n",
      "Epoch 108/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2345 - val_loss: 0.1242\n",
      "Epoch 109/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2363 - val_loss: 0.1302\n",
      "Epoch 110/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2317 - val_loss: 0.1185\n",
      "Epoch 111/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2305 - val_loss: 0.1219\n",
      "Epoch 112/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2297 - val_loss: 0.1216\n",
      "Epoch 113/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2321 - val_loss: 0.1173\n",
      "Epoch 114/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2302 - val_loss: 0.1338\n",
      "Epoch 115/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2321 - val_loss: 0.1308\n",
      "Epoch 116/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2340 - val_loss: 0.1340\n",
      "Epoch 117/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2394 - val_loss: 1.8222\n",
      "Epoch 118/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2416 - val_loss: 0.2230\n",
      "Epoch 119/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2334 - val_loss: 0.1270\n",
      "Epoch 120/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2309 - val_loss: 0.1197\n",
      "Epoch 121/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2323 - val_loss: 0.1248\n",
      "Epoch 122/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2311 - val_loss: 0.1218\n",
      "Epoch 123/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2316 - val_loss: 0.1127\n",
      "Epoch 124/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2311 - val_loss: 0.1286\n",
      "Epoch 125/200\n",
      "30354/30354 [==============================] - 2s 64us/sample - loss: 0.2301 - val_loss: 0.1441\n",
      "Epoch 126/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2306 - val_loss: 0.1263\n",
      "Epoch 127/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2352 - val_loss: 0.1220\n",
      "Epoch 128/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2327 - val_loss: 0.1353\n",
      "Epoch 129/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2334 - val_loss: 0.1379\n",
      "Epoch 130/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2331 - val_loss: 0.1240\n",
      "Epoch 131/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2341 - val_loss: 0.1259\n",
      "Epoch 132/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2352 - val_loss: 0.1314\n",
      "Epoch 133/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2334 - val_loss: 0.1258\n",
      "Epoch 134/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2339 - val_loss: 0.1341\n",
      "Epoch 135/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2361 - val_loss: 0.1277\n",
      "Epoch 136/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2342 - val_loss: 0.1205\n",
      "Epoch 137/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2333 - val_loss: 0.2087\n",
      "Epoch 138/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2339 - val_loss: 0.1587\n",
      "Epoch 139/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2332 - val_loss: 0.1974\n",
      "Epoch 140/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2359 - val_loss: 0.1511\n",
      "Epoch 141/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2337 - val_loss: 0.1588\n",
      "Epoch 142/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2344 - val_loss: 0.1604\n",
      "Epoch 143/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2319 - val_loss: 0.1649\n",
      "Epoch 144/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2334 - val_loss: 0.1365\n",
      "Epoch 145/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2335 - val_loss: 0.1993\n",
      "Epoch 146/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2348 - val_loss: 0.1522\n",
      "Epoch 147/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2385 - val_loss: 0.1188\n",
      "Epoch 148/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2345 - val_loss: 0.1285\n",
      "Epoch 149/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2310 - val_loss: 0.1271\n",
      "Epoch 150/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2309 - val_loss: 0.1267\n",
      "Epoch 151/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2316 - val_loss: 0.1219\n",
      "Epoch 152/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2346 - val_loss: 0.1199\n",
      "Epoch 153/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2322 - val_loss: 0.1164\n",
      "Epoch 154/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2320 - val_loss: 0.1220\n",
      "Epoch 155/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2326 - val_loss: 0.1240\n",
      "Epoch 156/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2352 - val_loss: 0.1403\n",
      "Epoch 157/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2314 - val_loss: 0.1396\n",
      "Epoch 158/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2338 - val_loss: 0.1178\n",
      "Epoch 159/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2339 - val_loss: 0.1292\n",
      "Epoch 160/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2349 - val_loss: 0.1236\n",
      "Epoch 161/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2317 - val_loss: 0.1549\n",
      "Epoch 162/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2324 - val_loss: 0.1509\n",
      "Epoch 163/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2355 - val_loss: 0.1220\n",
      "Epoch 164/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2303 - val_loss: 0.1363\n",
      "Epoch 165/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2307 - val_loss: 0.1237\n",
      "Epoch 166/200\n",
      "30354/30354 [==============================] - 2s 63us/sample - loss: 0.2322 - val_loss: 0.1182\n",
      "Epoch 167/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2332 - val_loss: 0.1295\n",
      "Epoch 168/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2335 - val_loss: 0.1673\n",
      "Epoch 169/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2356 - val_loss: 0.1488\n",
      "Epoch 170/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2353 - val_loss: 0.1424\n",
      "Epoch 171/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2327 - val_loss: 0.1279\n",
      "Epoch 172/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2315 - val_loss: 0.1342\n",
      "Epoch 173/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2326 - val_loss: 0.1280\n",
      "Epoch 174/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2323 - val_loss: 0.1259\n",
      "Epoch 175/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2331 - val_loss: 0.1172\n",
      "Epoch 176/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2327 - val_loss: 0.1373\n",
      "Epoch 177/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2326 - val_loss: 0.1221\n",
      "Epoch 178/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2317 - val_loss: 0.1254\n",
      "Epoch 179/200\n",
      "30354/30354 [==============================] - 2s 60us/sample - loss: 0.2322 - val_loss: 0.1291\n",
      "Epoch 180/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2358 - val_loss: 0.1293\n",
      "Epoch 181/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2341 - val_loss: 0.1219\n",
      "Epoch 182/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2328 - val_loss: 0.1467\n",
      "Epoch 183/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2311 - val_loss: 0.1483\n",
      "Epoch 184/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2378 - val_loss: 0.1350\n",
      "Epoch 185/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2325 - val_loss: 0.1158\n",
      "Epoch 186/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2301 - val_loss: 0.1129\n",
      "Epoch 187/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2300 - val_loss: 0.1275\n",
      "Epoch 188/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2337 - val_loss: 0.1208\n",
      "Epoch 189/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2307 - val_loss: 0.1232\n",
      "Epoch 190/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2291 - val_loss: 0.1444\n",
      "Epoch 191/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2311 - val_loss: 0.1262\n",
      "Epoch 192/200\n",
      "30354/30354 [==============================] - 2s 60us/sample - loss: 0.2326 - val_loss: 0.1360\n",
      "Epoch 193/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2322 - val_loss: 0.1251\n",
      "Epoch 194/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2338 - val_loss: 0.1433\n",
      "Epoch 195/200\n",
      "30354/30354 [==============================] - 2s 60us/sample - loss: 0.2348 - val_loss: 0.1323\n",
      "Epoch 196/200\n",
      "30354/30354 [==============================] - 2s 60us/sample - loss: 0.2343 - val_loss: 0.1201\n",
      "Epoch 197/200\n",
      "30354/30354 [==============================] - 2s 60us/sample - loss: 0.2348 - val_loss: 0.1214\n",
      "Epoch 198/200\n",
      "30354/30354 [==============================] - 2s 60us/sample - loss: 0.2327 - val_loss: 0.1292\n",
      "Epoch 199/200\n",
      "30354/30354 [==============================] - 2s 61us/sample - loss: 0.2339 - val_loss: 0.1189\n",
      "Epoch 200/200\n",
      "30354/30354 [==============================] - 2s 62us/sample - loss: 0.2306 - val_loss: 0.1465\n",
      "Training 1140674 samples for 2JHC\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 798471 samples, validate on 342203 samples\n",
      "Epoch 1/200\n",
      "798471/798471 [==============================] - 61s 76us/sample - loss: 0.7053 - val_loss: 0.4849\n",
      "Epoch 2/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.6078 - val_loss: 0.5748\n",
      "Epoch 3/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.5781 - val_loss: 0.4870\n",
      "Epoch 4/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.5528 - val_loss: 0.4817\n",
      "Epoch 5/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.5407 - val_loss: 0.8428\n",
      "Epoch 6/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.5142 - val_loss: 0.4197\n",
      "Epoch 7/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.5005 - val_loss: 0.4026\n",
      "Epoch 8/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4869 - val_loss: 0.3946\n",
      "Epoch 9/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4747 - val_loss: 0.3897\n",
      "Epoch 10/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4595 - val_loss: 0.3979\n",
      "Epoch 11/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4475 - val_loss: 0.3649\n",
      "Epoch 12/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4412 - val_loss: 0.3721\n",
      "Epoch 13/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4315 - val_loss: 0.3503\n",
      "Epoch 14/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4248 - val_loss: 0.4147\n",
      "Epoch 15/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4449 - val_loss: 0.5228\n",
      "Epoch 16/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4317 - val_loss: 0.4302\n",
      "Epoch 17/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4240 - val_loss: 0.4756\n",
      "Epoch 18/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4343 - val_loss: 0.4206\n",
      "Epoch 19/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.4441 - val_loss: 0.5420\n",
      "Epoch 20/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4297 - val_loss: 0.6809\n",
      "Epoch 21/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4239 - val_loss: 0.3491\n",
      "Epoch 22/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4100 - val_loss: 0.3540\n",
      "Epoch 23/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.4096 - val_loss: 0.5384\n",
      "Epoch 24/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.4094 - val_loss: 0.6302\n",
      "Epoch 25/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.4087 - val_loss: 0.4093\n",
      "Epoch 26/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4046 - val_loss: 0.4061\n",
      "Epoch 27/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4004 - val_loss: 0.3554\n",
      "Epoch 28/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3992 - val_loss: 1.9242\n",
      "Epoch 29/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3956 - val_loss: 0.3660\n",
      "Epoch 30/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.4026 - val_loss: 0.4632\n",
      "Epoch 31/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3953 - val_loss: 0.4477\n",
      "Epoch 32/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3905 - val_loss: 0.6835\n",
      "Epoch 33/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3890 - val_loss: 0.4431\n",
      "Epoch 34/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3877 - val_loss: 0.4571\n",
      "Epoch 35/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3875 - val_loss: 0.6848\n",
      "Epoch 36/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3846 - val_loss: 0.5089\n",
      "Epoch 37/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3872 - val_loss: 0.3825\n",
      "Epoch 38/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3836 - val_loss: 0.3847\n",
      "Epoch 39/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3788 - val_loss: 0.6691\n",
      "Epoch 40/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3822 - val_loss: 1.1970\n",
      "Epoch 41/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3797 - val_loss: 0.7554\n",
      "Epoch 42/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3779 - val_loss: 1.0371\n",
      "Epoch 43/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3770 - val_loss: 0.4830\n",
      "Epoch 44/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3759 - val_loss: 1.7884\n",
      "Epoch 45/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3753 - val_loss: 0.5387\n",
      "Epoch 46/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3766 - val_loss: 1.4938\n",
      "Epoch 47/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3743 - val_loss: 0.6413\n",
      "Epoch 48/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3725 - val_loss: 0.4124\n",
      "Epoch 49/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3736 - val_loss: 0.6687\n",
      "Epoch 50/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3719 - val_loss: 0.4421\n",
      "Epoch 51/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3701 - val_loss: 0.4816\n",
      "Epoch 52/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3689 - val_loss: 0.3582\n",
      "Epoch 53/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3661 - val_loss: 274.4131\n",
      "Epoch 54/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3656 - val_loss: 0.4474\n",
      "Epoch 55/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3641 - val_loss: 1.1469\n",
      "Epoch 56/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3644 - val_loss: 5.2836\n",
      "Epoch 57/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3613 - val_loss: 19.6346\n",
      "Epoch 58/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3599 - val_loss: 108.5278\n",
      "Epoch 59/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3579 - val_loss: 2239.9092\n",
      "Epoch 60/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3598 - val_loss: 35.4454\n",
      "Epoch 61/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3599 - val_loss: 3.9286\n",
      "Epoch 62/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3575 - val_loss: 1.0962\n",
      "Epoch 63/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3566 - val_loss: 1.1830\n",
      "Epoch 64/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3583 - val_loss: 1.1577\n",
      "Epoch 65/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3555 - val_loss: 2.9762\n",
      "Epoch 66/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3579 - val_loss: 3.0274\n",
      "Epoch 67/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3558 - val_loss: 5.7773\n",
      "Epoch 68/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3563 - val_loss: 1.8865\n",
      "Epoch 69/200\n",
      "798471/798471 [==============================] - 50s 63us/sample - loss: 0.3563 - val_loss: 0.7684\n",
      "Epoch 70/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3573 - val_loss: 0.2961\n",
      "Epoch 71/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3564 - val_loss: 0.3519\n",
      "Epoch 72/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3537 - val_loss: 0.8075\n",
      "Epoch 73/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3551 - val_loss: 7.4123\n",
      "Epoch 74/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3535 - val_loss: 27.7159\n",
      "Epoch 75/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3540 - val_loss: 100.1948\n",
      "Epoch 76/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3562 - val_loss: 2.4794\n",
      "Epoch 77/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3529 - val_loss: 0.6618\n",
      "Epoch 78/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3511 - val_loss: 2.5406\n",
      "Epoch 79/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3525 - val_loss: 1.3053\n",
      "Epoch 80/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3500 - val_loss: 1.0769\n",
      "Epoch 81/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3563 - val_loss: 1.0445\n",
      "Epoch 82/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3520 - val_loss: 1.1123\n",
      "Epoch 83/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3501 - val_loss: 0.7955\n",
      "Epoch 84/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3500 - val_loss: 0.6200\n",
      "Epoch 85/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3498 - val_loss: 4.7549\n",
      "Epoch 86/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3502 - val_loss: 1.1089\n",
      "Epoch 87/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3514 - val_loss: 49.3380\n",
      "Epoch 88/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3484 - val_loss: 4.0317\n",
      "Epoch 89/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3498 - val_loss: 0.9520\n",
      "Epoch 90/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3489 - val_loss: 2.7175\n",
      "Epoch 91/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3497 - val_loss: 3.0037\n",
      "Epoch 92/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3504 - val_loss: 0.8493\n",
      "Epoch 93/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3464 - val_loss: 1.4132\n",
      "Epoch 94/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3462 - val_loss: 53.4817\n",
      "Epoch 95/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3465 - val_loss: 6.2700\n",
      "Epoch 96/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3459 - val_loss: 3.7198\n",
      "Epoch 97/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3445 - val_loss: 6.4904\n",
      "Epoch 98/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3440 - val_loss: 4964.9681\n",
      "Epoch 99/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3428 - val_loss: 9.9914\n",
      "Epoch 100/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3434 - val_loss: 1.6704\n",
      "Epoch 101/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3449 - val_loss: 6.8638\n",
      "Epoch 102/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3421 - val_loss: 1.0757\n",
      "Epoch 103/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3423 - val_loss: 2.3850\n",
      "Epoch 104/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3415 - val_loss: 0.8212\n",
      "Epoch 105/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3441 - val_loss: 2.2283\n",
      "Epoch 106/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3417 - val_loss: 24.5744\n",
      "Epoch 107/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3414 - val_loss: 4515.2192\n",
      "Epoch 108/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3397 - val_loss: 8884.3352\n",
      "Epoch 109/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3425 - val_loss: 0.4093\n",
      "Epoch 110/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3446 - val_loss: 2093.6061\n",
      "Epoch 111/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3419 - val_loss: 3.3291\n",
      "Epoch 112/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3409 - val_loss: 228.6501\n",
      "Epoch 113/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3402 - val_loss: 75.2350\n",
      "Epoch 114/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3392 - val_loss: 67.7807\n",
      "Epoch 115/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3407 - val_loss: 9.3486\n",
      "Epoch 116/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3406 - val_loss: 5.7590\n",
      "Epoch 117/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3369 - val_loss: 245.3558\n",
      "Epoch 118/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3378 - val_loss: 12.3197\n",
      "Epoch 119/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3390 - val_loss: 8.2035\n",
      "Epoch 120/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3377 - val_loss: 8.6194\n",
      "Epoch 121/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3366 - val_loss: 2121.3191\n",
      "Epoch 122/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3366 - val_loss: 180.2006\n",
      "Epoch 123/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3400 - val_loss: 1.8981\n",
      "Epoch 124/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3373 - val_loss: 71.4529\n",
      "Epoch 125/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3352 - val_loss: 1477.0878\n",
      "Epoch 126/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3359 - val_loss: 23.8569\n",
      "Epoch 127/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3363 - val_loss: 478.9965\n",
      "Epoch 128/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3337 - val_loss: 6.2856\n",
      "Epoch 129/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3410 - val_loss: 32.5207\n",
      "Epoch 130/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3373 - val_loss: 11988.2559\n",
      "Epoch 131/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3339 - val_loss: 363.5816\n",
      "Epoch 132/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3357 - val_loss: 2.6273\n",
      "Epoch 133/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3369 - val_loss: 19.0329\n",
      "Epoch 134/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3338 - val_loss: 3.8707\n",
      "Epoch 135/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3407 - val_loss: 2.4642\n",
      "Epoch 136/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3371 - val_loss: 231.5355\n",
      "Epoch 137/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3366 - val_loss: 25.3558\n",
      "Epoch 138/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3458 - val_loss: 21.8236\n",
      "Epoch 139/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3400 - val_loss: 7.4082\n",
      "Epoch 140/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3370 - val_loss: 10.2351\n",
      "Epoch 141/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3365 - val_loss: 9.5922\n",
      "Epoch 142/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3343 - val_loss: 14.1984\n",
      "Epoch 143/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3344 - val_loss: 0.8743\n",
      "Epoch 144/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3395 - val_loss: 3.9523\n",
      "Epoch 145/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3378 - val_loss: 2.6787\n",
      "Epoch 146/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3360 - val_loss: 3.4585\n",
      "Epoch 147/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3372 - val_loss: 42.3884\n",
      "Epoch 148/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3365 - val_loss: 13.3057\n",
      "Epoch 149/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3342 - val_loss: 11.4156\n",
      "Epoch 150/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3336 - val_loss: 18.5600\n",
      "Epoch 151/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3348 - val_loss: 11.4814\n",
      "Epoch 152/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3340 - val_loss: 6.9580\n",
      "Epoch 153/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3368 - val_loss: 12.8263\n",
      "Epoch 154/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3352 - val_loss: 6.6621\n",
      "Epoch 155/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3344 - val_loss: 15.0168\n",
      "Epoch 156/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3352 - val_loss: 90.4944\n",
      "Epoch 157/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3348 - val_loss: 24.8135\n",
      "Epoch 158/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3339 - val_loss: 17.6371\n",
      "Epoch 159/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3322 - val_loss: 1.5256\n",
      "Epoch 160/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3350 - val_loss: 4.9965\n",
      "Epoch 161/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3321 - val_loss: 7.2215\n",
      "Epoch 162/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3306 - val_loss: 2.1722\n",
      "Epoch 163/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3318 - val_loss: 1.6965\n",
      "Epoch 164/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3317 - val_loss: 5.5757\n",
      "Epoch 165/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3305 - val_loss: 3.2656\n",
      "Epoch 166/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3316 - val_loss: 3.6418\n",
      "Epoch 167/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3331 - val_loss: 8.1040\n",
      "Epoch 168/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3357 - val_loss: 7.7165\n",
      "Epoch 169/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3312 - val_loss: 11.2496\n",
      "Epoch 170/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3294 - val_loss: 1.8858\n",
      "Epoch 171/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3312 - val_loss: 1.6796\n",
      "Epoch 172/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3351 - val_loss: 1.9540\n",
      "Epoch 173/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3333 - val_loss: 1.0870\n",
      "Epoch 174/200\n",
      "798471/798471 [==============================] - 50s 62us/sample - loss: 0.3322 - val_loss: 107.4297\n",
      "Epoch 175/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3332 - val_loss: 13.1748\n",
      "Epoch 176/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3396 - val_loss: 9.5771\n",
      "Epoch 177/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3347 - val_loss: 3.6969\n",
      "Epoch 178/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3342 - val_loss: 1.0800\n",
      "Epoch 179/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3338 - val_loss: 3.3512\n",
      "Epoch 180/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3326 - val_loss: 1.3091\n",
      "Epoch 181/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3334 - val_loss: 2.2983\n",
      "Epoch 182/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3336 - val_loss: 1.5426\n",
      "Epoch 183/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3299 - val_loss: 1.7668\n",
      "Epoch 184/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3310 - val_loss: 0.7374\n",
      "Epoch 185/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3300 - val_loss: 100.2709\n",
      "Epoch 186/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3292 - val_loss: 18.0472\n",
      "Epoch 187/200\n",
      "798471/798471 [==============================] - 49s 61us/sample - loss: 0.3302 - val_loss: 5.8855\n",
      "Epoch 188/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3298 - val_loss: 9.7097\n",
      "Epoch 189/200\n",
      "798471/798471 [==============================] - 49s 61us/sample - loss: 0.3292 - val_loss: 42.5762\n",
      "Epoch 190/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3301 - val_loss: 2.1984\n",
      "Epoch 191/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3333 - val_loss: 429.5656\n",
      "Epoch 192/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3329 - val_loss: 20.3856\n",
      "Epoch 193/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3308 - val_loss: 289.7023\n",
      "Epoch 194/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3290 - val_loss: 4.1636\n",
      "Epoch 195/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3321 - val_loss: 31.1466\n",
      "Epoch 196/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3331 - val_loss: 409.4414\n",
      "Epoch 197/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3349 - val_loss: 623.1942\n",
      "Epoch 198/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3312 - val_loss: 33940.4683\n",
      "Epoch 199/200\n",
      "798471/798471 [==============================] - 49s 62us/sample - loss: 0.3310 - val_loss: 48.3000\n",
      "Epoch 200/200\n",
      "798471/798471 [==============================] - 49s 61us/sample - loss: 0.3282 - val_loss: 688.1369\n",
      "Training 378036 samples for 2JHH\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 264625 samples, validate on 113411 samples\n",
      "Epoch 1/200\n",
      "264625/264625 [==============================] - 27s 103us/sample - loss: 0.8085 - val_loss: 0.8289\n",
      "Epoch 2/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.6647 - val_loss: 0.9616\n",
      "Epoch 3/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.6416 - val_loss: 0.6969\n",
      "Epoch 4/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.6260 - val_loss: 0.5718\n",
      "Epoch 5/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.6143 - val_loss: 0.6164\n",
      "Epoch 6/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.6029 - val_loss: 0.6355\n",
      "Epoch 7/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.6013 - val_loss: 0.5348\n",
      "Epoch 8/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5928 - val_loss: 0.4946\n",
      "Epoch 9/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5881 - val_loss: 0.6191\n",
      "Epoch 10/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5825 - val_loss: 0.5246\n",
      "Epoch 11/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5705 - val_loss: 0.4709\n",
      "Epoch 12/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5652 - val_loss: 0.5123\n",
      "Epoch 13/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5581 - val_loss: 0.5073\n",
      "Epoch 14/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5504 - val_loss: 0.4882\n",
      "Epoch 15/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5431 - val_loss: 0.5147\n",
      "Epoch 16/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5350 - val_loss: 0.5254\n",
      "Epoch 17/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5313 - val_loss: 0.4752\n",
      "Epoch 18/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5263 - val_loss: 0.4496\n",
      "Epoch 19/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5223 - val_loss: 0.4298\n",
      "Epoch 20/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.5184 - val_loss: 0.3752\n",
      "Epoch 21/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5149 - val_loss: 0.3410\n",
      "Epoch 22/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5106 - val_loss: 0.3149\n",
      "Epoch 23/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.5050 - val_loss: 0.3170\n",
      "Epoch 24/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4993 - val_loss: 0.3241\n",
      "Epoch 25/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4974 - val_loss: 0.3691\n",
      "Epoch 26/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4961 - val_loss: 0.3349\n",
      "Epoch 27/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4904 - val_loss: 0.3222\n",
      "Epoch 28/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4872 - val_loss: 0.2751\n",
      "Epoch 29/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4836 - val_loss: 0.2954\n",
      "Epoch 30/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4827 - val_loss: 0.2861\n",
      "Epoch 31/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4801 - val_loss: 0.3006\n",
      "Epoch 32/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4769 - val_loss: 0.3114\n",
      "Epoch 33/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4744 - val_loss: 0.3055\n",
      "Epoch 34/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4719 - val_loss: 0.3177\n",
      "Epoch 35/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4707 - val_loss: 0.3015\n",
      "Epoch 36/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4686 - val_loss: 0.2941\n",
      "Epoch 37/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4687 - val_loss: 0.2927\n",
      "Epoch 38/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4678 - val_loss: 0.3023\n",
      "Epoch 39/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4652 - val_loss: 0.3201\n",
      "Epoch 40/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4636 - val_loss: 0.3002\n",
      "Epoch 41/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4632 - val_loss: 0.3029\n",
      "Epoch 42/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4610 - val_loss: 0.2987\n",
      "Epoch 43/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4606 - val_loss: 0.3021\n",
      "Epoch 44/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4607 - val_loss: 0.2883\n",
      "Epoch 45/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4587 - val_loss: 0.2910\n",
      "Epoch 46/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4578 - val_loss: 0.3048\n",
      "Epoch 47/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4585 - val_loss: 0.2804\n",
      "Epoch 48/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4562 - val_loss: 0.3061\n",
      "Epoch 49/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4569 - val_loss: 0.2711\n",
      "Epoch 50/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4551 - val_loss: 0.2748\n",
      "Epoch 51/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4571 - val_loss: 0.2763\n",
      "Epoch 52/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4563 - val_loss: 0.2967\n",
      "Epoch 53/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4543 - val_loss: 0.2708\n",
      "Epoch 54/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4521 - val_loss: 0.2788\n",
      "Epoch 55/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4527 - val_loss: 0.2643\n",
      "Epoch 56/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4513 - val_loss: 0.2938\n",
      "Epoch 57/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4518 - val_loss: 0.2688\n",
      "Epoch 58/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4504 - val_loss: 0.2825\n",
      "Epoch 59/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4512 - val_loss: 0.2824\n",
      "Epoch 60/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4499 - val_loss: 0.2608\n",
      "Epoch 61/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4488 - val_loss: 0.2539\n",
      "Epoch 62/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4497 - val_loss: 0.2480\n",
      "Epoch 63/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4487 - val_loss: 0.2717\n",
      "Epoch 64/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4490 - val_loss: 0.2641\n",
      "Epoch 65/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4480 - val_loss: 0.2625\n",
      "Epoch 66/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4468 - val_loss: 0.2764\n",
      "Epoch 67/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4472 - val_loss: 0.2458\n",
      "Epoch 68/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4477 - val_loss: 0.2710\n",
      "Epoch 69/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4476 - val_loss: 0.2782\n",
      "Epoch 70/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4475 - val_loss: 0.2479\n",
      "Epoch 71/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4465 - val_loss: 0.2469\n",
      "Epoch 72/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4477 - val_loss: 0.2545\n",
      "Epoch 73/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4452 - val_loss: 0.2740\n",
      "Epoch 74/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4443 - val_loss: 0.2490\n",
      "Epoch 75/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4455 - val_loss: 0.2297\n",
      "Epoch 76/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4439 - val_loss: 0.2821\n",
      "Epoch 77/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4444 - val_loss: 0.2484\n",
      "Epoch 78/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4442 - val_loss: 0.2705\n",
      "Epoch 79/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4435 - val_loss: 0.2485\n",
      "Epoch 80/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4426 - val_loss: 0.2426\n",
      "Epoch 81/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4418 - val_loss: 0.2333\n",
      "Epoch 82/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4431 - val_loss: 0.2696\n",
      "Epoch 83/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4439 - val_loss: 0.2418\n",
      "Epoch 84/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4413 - val_loss: 0.2537\n",
      "Epoch 85/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4424 - val_loss: 0.2607\n",
      "Epoch 86/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4418 - val_loss: 0.2437\n",
      "Epoch 87/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4424 - val_loss: 0.2427\n",
      "Epoch 88/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4435 - val_loss: 0.2443\n",
      "Epoch 89/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4415 - val_loss: 0.2469\n",
      "Epoch 90/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4417 - val_loss: 0.2325\n",
      "Epoch 91/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4413 - val_loss: 0.2481\n",
      "Epoch 92/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4425 - val_loss: 0.2305\n",
      "Epoch 93/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4406 - val_loss: 0.2551\n",
      "Epoch 94/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4399 - val_loss: 0.2543\n",
      "Epoch 95/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4401 - val_loss: 0.2558\n",
      "Epoch 96/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4383 - val_loss: 0.2605\n",
      "Epoch 97/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4394 - val_loss: 0.2475\n",
      "Epoch 98/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4394 - val_loss: 0.2137\n",
      "Epoch 99/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4391 - val_loss: 0.2654\n",
      "Epoch 100/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4379 - val_loss: 0.2531\n",
      "Epoch 101/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4380 - val_loss: 0.2461\n",
      "Epoch 102/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4397 - val_loss: 0.2619\n",
      "Epoch 103/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4396 - val_loss: 0.2579\n",
      "Epoch 104/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4382 - val_loss: 0.2416\n",
      "Epoch 105/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4378 - val_loss: 0.2424\n",
      "Epoch 106/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4374 - val_loss: 0.2537\n",
      "Epoch 107/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4372 - val_loss: 0.2476\n",
      "Epoch 108/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4385 - val_loss: 0.2675\n",
      "Epoch 109/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4380 - val_loss: 0.2490\n",
      "Epoch 110/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4384 - val_loss: 0.2560\n",
      "Epoch 111/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4385 - val_loss: 0.2609\n",
      "Epoch 112/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4379 - val_loss: 0.2359\n",
      "Epoch 113/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4370 - val_loss: 0.2381\n",
      "Epoch 114/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4357 - val_loss: 0.2667\n",
      "Epoch 115/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4371 - val_loss: 0.2674\n",
      "Epoch 116/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4363 - val_loss: 0.2678\n",
      "Epoch 117/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4384 - val_loss: 0.2481\n",
      "Epoch 118/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4377 - val_loss: 0.2599\n",
      "Epoch 119/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4378 - val_loss: 0.2643\n",
      "Epoch 120/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4391 - val_loss: 0.2724\n",
      "Epoch 121/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4369 - val_loss: 0.2626\n",
      "Epoch 122/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4364 - val_loss: 0.2378\n",
      "Epoch 123/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4346 - val_loss: 0.2387\n",
      "Epoch 124/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4382 - val_loss: 0.2482\n",
      "Epoch 125/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4365 - val_loss: 0.2587\n",
      "Epoch 126/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4372 - val_loss: 0.2473\n",
      "Epoch 127/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4364 - val_loss: 0.2426\n",
      "Epoch 128/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4375 - val_loss: 0.2483\n",
      "Epoch 129/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4384 - val_loss: 0.2649\n",
      "Epoch 130/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4371 - val_loss: 0.2584\n",
      "Epoch 131/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4367 - val_loss: 0.2476\n",
      "Epoch 132/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4361 - val_loss: 0.2585\n",
      "Epoch 133/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4364 - val_loss: 0.2490\n",
      "Epoch 134/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4353 - val_loss: 0.2621\n",
      "Epoch 135/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4356 - val_loss: 0.2573\n",
      "Epoch 136/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4359 - val_loss: 0.2570\n",
      "Epoch 137/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4370 - val_loss: 0.2699\n",
      "Epoch 138/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4366 - val_loss: 0.2603\n",
      "Epoch 139/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4347 - val_loss: 0.2633\n",
      "Epoch 140/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4350 - val_loss: 0.2631\n",
      "Epoch 141/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4360 - val_loss: 0.2621\n",
      "Epoch 142/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4359 - val_loss: 0.2443\n",
      "Epoch 143/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4387 - val_loss: 0.2746\n",
      "Epoch 144/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4372 - val_loss: 0.2642\n",
      "Epoch 145/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4358 - val_loss: 0.2844\n",
      "Epoch 146/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4381 - val_loss: 0.2626\n",
      "Epoch 147/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4382 - val_loss: 0.2606\n",
      "Epoch 148/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4374 - val_loss: 0.2685\n",
      "Epoch 149/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4377 - val_loss: 0.2889\n",
      "Epoch 150/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4368 - val_loss: 0.2479\n",
      "Epoch 151/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4365 - val_loss: 0.2691\n",
      "Epoch 152/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4377 - val_loss: 0.2688\n",
      "Epoch 153/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4364 - val_loss: 0.2642\n",
      "Epoch 154/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4354 - val_loss: 0.2762\n",
      "Epoch 155/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4352 - val_loss: 0.2836\n",
      "Epoch 156/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4358 - val_loss: 0.2703\n",
      "Epoch 157/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4369 - val_loss: 0.2642\n",
      "Epoch 158/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4369 - val_loss: 0.2579\n",
      "Epoch 159/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4363 - val_loss: 0.2539\n",
      "Epoch 160/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4361 - val_loss: 0.2759\n",
      "Epoch 161/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4370 - val_loss: 0.2769\n",
      "Epoch 162/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4342 - val_loss: 0.2703\n",
      "Epoch 163/200\n",
      "264625/264625 [==============================] - 16s 60us/sample - loss: 0.4367 - val_loss: 0.2649\n",
      "Epoch 164/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4373 - val_loss: 0.2992\n",
      "Epoch 165/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4357 - val_loss: 0.2817\n",
      "Epoch 166/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4365 - val_loss: 0.2776\n",
      "Epoch 167/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4367 - val_loss: 0.2859\n",
      "Epoch 168/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4357 - val_loss: 0.2629\n",
      "Epoch 169/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4375 - val_loss: 0.2778\n",
      "Epoch 170/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4371 - val_loss: 0.3104\n",
      "Epoch 171/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4363 - val_loss: 0.3156\n",
      "Epoch 172/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4363 - val_loss: 0.3044\n",
      "Epoch 173/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4366 - val_loss: 0.3008\n",
      "Epoch 174/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4372 - val_loss: 0.2676\n",
      "Epoch 175/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4347 - val_loss: 0.2732\n",
      "Epoch 176/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4349 - val_loss: 0.2891\n",
      "Epoch 177/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4356 - val_loss: 0.3010\n",
      "Epoch 178/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4350 - val_loss: 0.2907\n",
      "Epoch 179/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4345 - val_loss: 0.2905\n",
      "Epoch 180/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4374 - val_loss: 0.2907\n",
      "Epoch 181/200\n",
      "264625/264625 [==============================] - 16s 62us/sample - loss: 0.4331 - val_loss: 0.2909\n",
      "Epoch 182/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4343 - val_loss: 0.3024\n",
      "Epoch 183/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4346 - val_loss: 0.3119\n",
      "Epoch 184/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4356 - val_loss: 0.3006\n",
      "Epoch 185/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4339 - val_loss: 0.2724\n",
      "Epoch 186/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4346 - val_loss: 0.2725\n",
      "Epoch 187/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4381 - val_loss: 0.2755\n",
      "Epoch 188/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4355 - val_loss: 0.2656\n",
      "Epoch 189/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4344 - val_loss: 0.2905\n",
      "Epoch 190/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4334 - val_loss: 0.2842\n",
      "Epoch 191/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4333 - val_loss: 0.2845\n",
      "Epoch 192/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4342 - val_loss: 0.2995\n",
      "Epoch 193/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4322 - val_loss: 0.2800\n",
      "Epoch 194/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4333 - val_loss: 0.2718\n",
      "Epoch 195/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4332 - val_loss: 0.2894\n",
      "Epoch 196/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4348 - val_loss: 0.2784\n",
      "Epoch 197/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4335 - val_loss: 0.3068\n",
      "Epoch 198/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4330 - val_loss: 0.2917\n",
      "Epoch 199/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4321 - val_loss: 0.2756\n",
      "Epoch 200/200\n",
      "264625/264625 [==============================] - 16s 61us/sample - loss: 0.4337 - val_loss: 0.2956\n",
      "Training 119253 samples for 2JHN\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 83477 samples, validate on 35776 samples\n",
      "Epoch 1/200\n",
      "83477/83477 [==============================] - 17s 200us/sample - loss: 0.6836 - val_loss: 5.4524\n",
      "Epoch 2/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.4077 - val_loss: 1.5566\n",
      "Epoch 3/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.3565 - val_loss: 0.4041\n",
      "Epoch 4/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.3300 - val_loss: 0.4817\n",
      "Epoch 5/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.3012 - val_loss: 0.7095\n",
      "Epoch 6/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2874 - val_loss: 0.4242\n",
      "Epoch 7/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2760 - val_loss: 0.3797\n",
      "Epoch 8/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2626 - val_loss: 0.2938\n",
      "Epoch 9/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2591 - val_loss: 0.7178\n",
      "Epoch 10/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2550 - val_loss: 0.2094\n",
      "Epoch 11/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2544 - val_loss: 0.4988\n",
      "Epoch 12/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2525 - val_loss: 0.4596\n",
      "Epoch 13/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2497 - val_loss: 0.5864\n",
      "Epoch 14/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2459 - val_loss: 0.3440\n",
      "Epoch 15/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2500 - val_loss: 0.5356\n",
      "Epoch 16/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2478 - val_loss: 0.1936\n",
      "Epoch 17/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2468 - val_loss: 0.4013\n",
      "Epoch 18/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2542 - val_loss: 0.5942\n",
      "Epoch 19/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2443 - val_loss: 0.4543\n",
      "Epoch 20/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2389 - val_loss: 0.7069\n",
      "Epoch 21/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2441 - val_loss: 0.1946\n",
      "Epoch 22/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2424 - val_loss: 0.1979\n",
      "Epoch 23/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2382 - val_loss: 0.6712\n",
      "Epoch 24/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2515 - val_loss: 0.1633\n",
      "Epoch 25/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2430 - val_loss: 0.1693\n",
      "Epoch 26/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2386 - val_loss: 0.1600\n",
      "Epoch 27/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2393 - val_loss: 0.1818\n",
      "Epoch 28/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2424 - val_loss: 0.8885\n",
      "Epoch 29/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2378 - val_loss: 0.3176\n",
      "Epoch 30/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2432 - val_loss: 0.3084\n",
      "Epoch 31/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2348 - val_loss: 0.8347\n",
      "Epoch 32/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2328 - val_loss: 0.2674\n",
      "Epoch 33/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2398 - val_loss: 0.4733\n",
      "Epoch 34/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2424 - val_loss: 0.2774\n",
      "Epoch 35/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.2327 - val_loss: 0.1763\n",
      "Epoch 36/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2307 - val_loss: 0.1531\n",
      "Epoch 37/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2363 - val_loss: 0.1722\n",
      "Epoch 38/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2363 - val_loss: 0.2318\n",
      "Epoch 39/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2394 - val_loss: 0.1711\n",
      "Epoch 40/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2380 - val_loss: 0.2496\n",
      "Epoch 41/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2348 - val_loss: 0.1660\n",
      "Epoch 42/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2311 - val_loss: 0.1688\n",
      "Epoch 43/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2364 - val_loss: 0.1773\n",
      "Epoch 44/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2312 - val_loss: 0.1587\n",
      "Epoch 45/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2315 - val_loss: 0.1639\n",
      "Epoch 46/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2339 - val_loss: 0.1827\n",
      "Epoch 47/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2375 - val_loss: 0.1812\n",
      "Epoch 48/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2337 - val_loss: 0.1745\n",
      "Epoch 49/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2290 - val_loss: 0.1520\n",
      "Epoch 50/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2228 - val_loss: 0.2012\n",
      "Epoch 51/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2214 - val_loss: 0.2124\n",
      "Epoch 52/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2179 - val_loss: 0.1806\n",
      "Epoch 53/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2167 - val_loss: 0.1749\n",
      "Epoch 54/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2138 - val_loss: 0.1728\n",
      "Epoch 55/200\n",
      "83477/83477 [==============================] - 5s 63us/sample - loss: 0.2128 - val_loss: 0.1884\n",
      "Epoch 56/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2121 - val_loss: 0.1635\n",
      "Epoch 57/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.2104 - val_loss: 0.1704\n",
      "Epoch 58/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.2101 - val_loss: 0.2035\n",
      "Epoch 59/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2086 - val_loss: 0.1610\n",
      "Epoch 60/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2087 - val_loss: 0.1606\n",
      "Epoch 61/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2076 - val_loss: 0.2035\n",
      "Epoch 62/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2076 - val_loss: 0.1927\n",
      "Epoch 63/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2058 - val_loss: 0.1819\n",
      "Epoch 64/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2052 - val_loss: 0.1629\n",
      "Epoch 65/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2049 - val_loss: 0.1820\n",
      "Epoch 66/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2059 - val_loss: 0.1664\n",
      "Epoch 67/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2089 - val_loss: 0.1573\n",
      "Epoch 68/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2127 - val_loss: 0.1548\n",
      "Epoch 69/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2083 - val_loss: 0.1770\n",
      "Epoch 70/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2073 - val_loss: 0.1552\n",
      "Epoch 71/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2074 - val_loss: 0.1698\n",
      "Epoch 72/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2040 - val_loss: 0.1447\n",
      "Epoch 73/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2032 - val_loss: 0.1596\n",
      "Epoch 74/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.2019 - val_loss: 0.1528\n",
      "Epoch 75/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2017 - val_loss: 0.1520\n",
      "Epoch 76/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2001 - val_loss: 0.1542\n",
      "Epoch 77/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.2010 - val_loss: 0.1408\n",
      "Epoch 78/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.2005 - val_loss: 0.1412\n",
      "Epoch 79/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.2007 - val_loss: 0.1372\n",
      "Epoch 80/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1995 - val_loss: 0.1400\n",
      "Epoch 81/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.1990 - val_loss: 0.1386\n",
      "Epoch 82/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1996 - val_loss: 0.1443\n",
      "Epoch 83/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1984 - val_loss: 0.1411\n",
      "Epoch 84/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1991 - val_loss: 0.1428\n",
      "Epoch 85/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1981 - val_loss: 0.1386\n",
      "Epoch 86/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1973 - val_loss: 0.1415\n",
      "Epoch 87/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1977 - val_loss: 0.1526\n",
      "Epoch 88/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1975 - val_loss: 0.1486\n",
      "Epoch 89/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.1977 - val_loss: 0.1557\n",
      "Epoch 90/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1975 - val_loss: 0.1423\n",
      "Epoch 91/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1973 - val_loss: 0.1361\n",
      "Epoch 92/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1964 - val_loss: 0.1343\n",
      "Epoch 93/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1972 - val_loss: 0.1349\n",
      "Epoch 94/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1963 - val_loss: 0.1471\n",
      "Epoch 95/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1960 - val_loss: 0.1504\n",
      "Epoch 96/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1958 - val_loss: 0.1410\n",
      "Epoch 97/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1954 - val_loss: 0.1370\n",
      "Epoch 98/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1949 - val_loss: 0.1373\n",
      "Epoch 99/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1945 - val_loss: 0.1455\n",
      "Epoch 100/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1952 - val_loss: 0.1322\n",
      "Epoch 101/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1955 - val_loss: 0.1405\n",
      "Epoch 102/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.1941 - val_loss: 0.1396\n",
      "Epoch 103/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1938 - val_loss: 0.1334\n",
      "Epoch 104/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1937 - val_loss: 0.1419\n",
      "Epoch 105/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1940 - val_loss: 0.1374\n",
      "Epoch 106/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1941 - val_loss: 0.1400\n",
      "Epoch 107/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1960 - val_loss: 0.1304\n",
      "Epoch 108/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1953 - val_loss: 0.1268\n",
      "Epoch 109/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1943 - val_loss: 0.1520\n",
      "Epoch 110/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1947 - val_loss: 0.1320\n",
      "Epoch 111/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1961 - val_loss: 0.1342\n",
      "Epoch 112/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1954 - val_loss: 0.1343\n",
      "Epoch 113/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1958 - val_loss: 0.1375\n",
      "Epoch 114/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1944 - val_loss: 0.1428\n",
      "Epoch 115/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1947 - val_loss: 0.1335\n",
      "Epoch 116/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1935 - val_loss: 0.1458\n",
      "Epoch 117/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1940 - val_loss: 0.1356\n",
      "Epoch 118/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1938 - val_loss: 0.1347\n",
      "Epoch 119/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1926 - val_loss: 0.1509\n",
      "Epoch 120/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1928 - val_loss: 0.1349\n",
      "Epoch 121/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1926 - val_loss: 0.1318\n",
      "Epoch 122/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1926 - val_loss: 0.1370\n",
      "Epoch 123/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.1915 - val_loss: 0.1472\n",
      "Epoch 124/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1923 - val_loss: 0.1370\n",
      "Epoch 125/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1918 - val_loss: 0.1404\n",
      "Epoch 126/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1919 - val_loss: 0.1523\n",
      "Epoch 127/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1929 - val_loss: 0.1397\n",
      "Epoch 128/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1926 - val_loss: 0.1581\n",
      "Epoch 129/200\n",
      "83477/83477 [==============================] - 5s 59us/sample - loss: 0.1914 - val_loss: 0.1395\n",
      "Epoch 130/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1912 - val_loss: 0.1382\n",
      "Epoch 131/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1919 - val_loss: 0.1416\n",
      "Epoch 132/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1917 - val_loss: 0.1487\n",
      "Epoch 133/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1916 - val_loss: 0.1377\n",
      "Epoch 134/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1912 - val_loss: 0.1386\n",
      "Epoch 135/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1916 - val_loss: 0.1427\n",
      "Epoch 136/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1913 - val_loss: 0.1725\n",
      "Epoch 137/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1933 - val_loss: 0.1619\n",
      "Epoch 138/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1912 - val_loss: 0.1711\n",
      "Epoch 139/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1913 - val_loss: 0.1461\n",
      "Epoch 140/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.1907 - val_loss: 0.1411\n",
      "Epoch 141/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1907 - val_loss: 0.1446\n",
      "Epoch 142/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1913 - val_loss: 0.1375\n",
      "Epoch 143/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1913 - val_loss: 0.1423\n",
      "Epoch 144/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1907 - val_loss: 0.1476\n",
      "Epoch 145/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1903 - val_loss: 0.1546\n",
      "Epoch 146/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1911 - val_loss: 0.1494\n",
      "Epoch 147/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1907 - val_loss: 0.1458\n",
      "Epoch 148/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1913 - val_loss: 0.1438\n",
      "Epoch 149/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1915 - val_loss: 0.1397\n",
      "Epoch 150/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1932 - val_loss: 0.1387\n",
      "Epoch 151/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1920 - val_loss: 0.1502\n",
      "Epoch 152/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1931 - val_loss: 0.1420\n",
      "Epoch 153/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1917 - val_loss: 0.1522\n",
      "Epoch 154/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1925 - val_loss: 0.1470\n",
      "Epoch 155/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1945 - val_loss: 0.1420\n",
      "Epoch 156/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1930 - val_loss: 0.1461\n",
      "Epoch 157/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1935 - val_loss: 0.1476\n",
      "Epoch 158/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1931 - val_loss: 0.1452\n",
      "Epoch 159/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1941 - val_loss: 0.1610\n",
      "Epoch 160/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1934 - val_loss: 0.1613\n",
      "Epoch 161/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1927 - val_loss: 0.1655\n",
      "Epoch 162/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1938 - val_loss: 0.1804\n",
      "Epoch 163/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1949 - val_loss: 0.1831\n",
      "Epoch 164/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1970 - val_loss: 0.1860\n",
      "Epoch 165/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1941 - val_loss: 0.1959\n",
      "Epoch 166/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1962 - val_loss: 0.1675\n",
      "Epoch 167/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1956 - val_loss: 0.2104\n",
      "Epoch 168/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1951 - val_loss: 0.1746\n",
      "Epoch 169/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1930 - val_loss: 0.1821\n",
      "Epoch 170/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1928 - val_loss: 0.1591\n",
      "Epoch 171/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1924 - val_loss: 0.1713\n",
      "Epoch 172/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1936 - val_loss: 0.1657\n",
      "Epoch 173/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1916 - val_loss: 0.1646\n",
      "Epoch 174/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1922 - val_loss: 0.1639\n",
      "Epoch 175/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1920 - val_loss: 0.1707\n",
      "Epoch 176/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1925 - val_loss: 0.1651\n",
      "Epoch 177/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1924 - val_loss: 0.1594\n",
      "Epoch 178/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1909 - val_loss: 0.1943\n",
      "Epoch 179/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1912 - val_loss: 0.1710\n",
      "Epoch 180/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1918 - val_loss: 0.1708\n",
      "Epoch 181/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1918 - val_loss: 0.1695\n",
      "Epoch 182/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1942 - val_loss: 0.2035\n",
      "Epoch 183/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1929 - val_loss: 0.1671\n",
      "Epoch 184/200\n",
      "83477/83477 [==============================] - 5s 62us/sample - loss: 0.1906 - val_loss: 0.1706\n",
      "Epoch 185/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1914 - val_loss: 0.1820\n",
      "Epoch 186/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1927 - val_loss: 0.1660\n",
      "Epoch 187/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1908 - val_loss: 0.1576\n",
      "Epoch 188/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1920 - val_loss: 0.1523\n",
      "Epoch 189/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1910 - val_loss: 0.1409\n",
      "Epoch 190/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1913 - val_loss: 0.1430\n",
      "Epoch 191/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1905 - val_loss: 0.1504\n",
      "Epoch 192/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1902 - val_loss: 0.1449\n",
      "Epoch 193/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1888 - val_loss: 0.1711\n",
      "Epoch 194/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1901 - val_loss: 0.1586\n",
      "Epoch 195/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1904 - val_loss: 0.1850\n",
      "Epoch 196/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1894 - val_loss: 0.1592\n",
      "Epoch 197/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1912 - val_loss: 0.1505\n",
      "Epoch 198/200\n",
      "83477/83477 [==============================] - 5s 61us/sample - loss: 0.1912 - val_loss: 0.1471\n",
      "Epoch 199/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1904 - val_loss: 0.1626\n",
      "Epoch 200/200\n",
      "83477/83477 [==============================] - 5s 60us/sample - loss: 0.1909 - val_loss: 0.1614\n",
      "Training 1510379 samples for 3JHC\n",
      "  Setting up data\n",
      "[12, 2, 7] 12 7 \n",
      " dsgdb9nsd_133831\n",
      "[17, 7, 2] 17 2 \n",
      " dsgdb9nsd_133831\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 1057265 samples, validate on 453114 samples\n",
      "Epoch 1/200\n",
      "1057265/1057265 [==============================] - 80s 75us/sample - loss: 0.3642 - val_loss: 0.3068\n",
      "Epoch 2/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.2745 - val_loss: 0.2294\n",
      "Epoch 3/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.2579 - val_loss: 0.2270\n",
      "Epoch 4/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.2526 - val_loss: 0.2200\n",
      "Epoch 5/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.2418 - val_loss: 0.2111\n",
      "Epoch 6/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.2293 - val_loss: 0.2056\n",
      "Epoch 7/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.2175 - val_loss: 0.2003\n",
      "Epoch 8/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.2113 - val_loss: 0.1903\n",
      "Epoch 9/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.2044 - val_loss: 0.1858\n",
      "Epoch 10/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1992 - val_loss: 0.1844\n",
      "Epoch 11/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1949 - val_loss: 0.1778\n",
      "Epoch 12/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1913 - val_loss: 0.1740\n",
      "Epoch 13/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1875 - val_loss: 0.1749\n",
      "Epoch 14/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1851 - val_loss: 0.1705\n",
      "Epoch 15/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1823 - val_loss: 0.1669\n",
      "Epoch 16/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1799 - val_loss: 0.1647\n",
      "Epoch 17/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1775 - val_loss: 0.1633\n",
      "Epoch 18/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1757 - val_loss: 0.1612\n",
      "Epoch 19/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1737 - val_loss: 0.1578\n",
      "Epoch 20/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1727 - val_loss: 0.1584\n",
      "Epoch 21/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1704 - val_loss: 0.1522\n",
      "Epoch 22/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1691 - val_loss: 0.1506\n",
      "Epoch 23/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1677 - val_loss: 0.1503\n",
      "Epoch 24/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1666 - val_loss: 0.1551\n",
      "Epoch 25/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1653 - val_loss: 0.1468\n",
      "Epoch 26/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1641 - val_loss: 0.1462\n",
      "Epoch 27/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1628 - val_loss: 0.1467\n",
      "Epoch 28/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1621 - val_loss: 0.1400\n",
      "Epoch 29/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1610 - val_loss: 0.1449\n",
      "Epoch 30/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1604 - val_loss: 0.1412\n",
      "Epoch 31/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1591 - val_loss: 0.1424\n",
      "Epoch 32/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1583 - val_loss: 0.1377\n",
      "Epoch 33/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1582 - val_loss: 0.1446\n",
      "Epoch 34/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1570 - val_loss: 0.1385\n",
      "Epoch 35/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1563 - val_loss: 0.1391\n",
      "Epoch 36/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1557 - val_loss: 0.1375\n",
      "Epoch 37/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1550 - val_loss: 0.1362\n",
      "Epoch 38/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1547 - val_loss: 0.1321\n",
      "Epoch 39/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1540 - val_loss: 0.1353\n",
      "Epoch 40/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1535 - val_loss: 0.1351\n",
      "Epoch 41/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1527 - val_loss: 0.1347\n",
      "Epoch 42/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1524 - val_loss: 0.1339\n",
      "Epoch 43/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1521 - val_loss: 0.1413\n",
      "Epoch 44/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1518 - val_loss: 0.1349\n",
      "Epoch 45/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1512 - val_loss: 0.1356\n",
      "Epoch 46/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1508 - val_loss: 0.1310\n",
      "Epoch 47/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1504 - val_loss: 0.1347\n",
      "Epoch 48/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1503 - val_loss: 0.1305\n",
      "Epoch 49/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1498 - val_loss: 0.1337\n",
      "Epoch 50/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1493 - val_loss: 0.1317\n",
      "Epoch 51/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1489 - val_loss: 0.1352\n",
      "Epoch 52/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1489 - val_loss: 0.1384\n",
      "Epoch 53/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1486 - val_loss: 0.1366\n",
      "Epoch 54/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1481 - val_loss: 0.1283\n",
      "Epoch 55/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1477 - val_loss: 0.1317\n",
      "Epoch 56/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1473 - val_loss: 0.1294\n",
      "Epoch 57/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1467 - val_loss: 0.1305\n",
      "Epoch 58/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1468 - val_loss: 0.1302\n",
      "Epoch 59/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1465 - val_loss: 0.1305\n",
      "Epoch 60/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1460 - val_loss: 0.1288\n",
      "Epoch 61/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1460 - val_loss: 0.1320\n",
      "Epoch 62/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1459 - val_loss: 0.1261\n",
      "Epoch 63/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1458 - val_loss: 0.1243\n",
      "Epoch 64/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1454 - val_loss: 0.1281\n",
      "Epoch 65/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1450 - val_loss: 0.1351\n",
      "Epoch 66/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1447 - val_loss: 0.1341\n",
      "Epoch 67/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1445 - val_loss: 0.1317\n",
      "Epoch 68/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1444 - val_loss: 0.1229\n",
      "Epoch 69/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1438 - val_loss: 0.1274\n",
      "Epoch 70/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1436 - val_loss: 0.1339\n",
      "Epoch 71/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1433 - val_loss: 0.1377\n",
      "Epoch 72/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1433 - val_loss: 0.1255\n",
      "Epoch 73/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1429 - val_loss: 0.1294\n",
      "Epoch 74/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1429 - val_loss: 0.1331\n",
      "Epoch 75/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1428 - val_loss: 0.1288\n",
      "Epoch 76/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1426 - val_loss: 0.1299\n",
      "Epoch 77/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1423 - val_loss: 0.1252\n",
      "Epoch 78/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1422 - val_loss: 0.1255\n",
      "Epoch 79/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1417 - val_loss: 0.1297\n",
      "Epoch 80/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1423 - val_loss: 0.1249\n",
      "Epoch 81/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1417 - val_loss: 0.1268\n",
      "Epoch 82/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1418 - val_loss: 0.1302\n",
      "Epoch 83/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1414 - val_loss: 0.1238\n",
      "Epoch 84/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1411 - val_loss: 0.1271\n",
      "Epoch 85/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1410 - val_loss: 0.1277\n",
      "Epoch 86/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1408 - val_loss: 0.1282\n",
      "Epoch 87/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1411 - val_loss: 0.1317\n",
      "Epoch 88/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1407 - val_loss: 0.1291\n",
      "Epoch 89/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1414 - val_loss: 0.1199\n",
      "Epoch 90/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1416 - val_loss: 0.1204\n",
      "Epoch 91/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1409 - val_loss: 0.1212\n",
      "Epoch 92/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1413 - val_loss: 0.1211\n",
      "Epoch 93/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1413 - val_loss: 0.1228\n",
      "Epoch 94/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1415 - val_loss: 0.1164\n",
      "Epoch 95/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1404 - val_loss: 0.1178\n",
      "Epoch 96/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1400 - val_loss: 0.1198\n",
      "Epoch 97/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1403 - val_loss: 0.1181\n",
      "Epoch 98/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1395 - val_loss: 0.1170\n",
      "Epoch 99/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1393 - val_loss: 0.1174\n",
      "Epoch 100/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1396 - val_loss: 0.1161\n",
      "Epoch 101/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1403 - val_loss: 0.1161\n",
      "Epoch 102/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1401 - val_loss: 0.1182\n",
      "Epoch 103/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1399 - val_loss: 0.1212\n",
      "Epoch 104/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1399 - val_loss: 0.1171\n",
      "Epoch 105/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1400 - val_loss: 0.1201\n",
      "Epoch 106/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1391 - val_loss: 0.1195\n",
      "Epoch 107/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1389 - val_loss: 0.1200\n",
      "Epoch 108/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1386 - val_loss: 0.1158\n",
      "Epoch 109/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1383 - val_loss: 0.1182\n",
      "Epoch 110/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1381 - val_loss: 0.1166\n",
      "Epoch 111/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1380 - val_loss: 0.1176\n",
      "Epoch 112/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1381 - val_loss: 0.1143\n",
      "Epoch 113/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1377 - val_loss: 0.1146\n",
      "Epoch 114/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1376 - val_loss: 0.1161\n",
      "Epoch 115/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1375 - val_loss: 0.1136\n",
      "Epoch 116/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1380 - val_loss: 0.1161\n",
      "Epoch 117/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1379 - val_loss: 0.1190\n",
      "Epoch 118/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1378 - val_loss: 0.1185\n",
      "Epoch 119/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1376 - val_loss: 0.1195\n",
      "Epoch 120/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1378 - val_loss: 0.1194\n",
      "Epoch 121/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1375 - val_loss: 0.1219\n",
      "Epoch 122/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1377 - val_loss: 0.1184\n",
      "Epoch 123/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1381 - val_loss: 0.1162\n",
      "Epoch 124/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1375 - val_loss: 0.1177\n",
      "Epoch 125/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1376 - val_loss: 0.1182\n",
      "Epoch 126/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1375 - val_loss: 0.1174\n",
      "Epoch 127/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1375 - val_loss: 0.1149\n",
      "Epoch 128/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1370 - val_loss: 0.1172\n",
      "Epoch 129/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1370 - val_loss: 0.1177\n",
      "Epoch 130/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1374 - val_loss: 0.1152\n",
      "Epoch 131/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1369 - val_loss: 0.1170\n",
      "Epoch 132/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1369 - val_loss: 0.1141\n",
      "Epoch 133/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1371 - val_loss: 0.1149\n",
      "Epoch 134/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1371 - val_loss: 0.1170\n",
      "Epoch 135/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1380 - val_loss: 0.1192\n",
      "Epoch 136/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1377 - val_loss: 0.1172\n",
      "Epoch 137/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1371 - val_loss: 0.1176\n",
      "Epoch 138/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1375 - val_loss: 0.1221\n",
      "Epoch 139/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1363 - val_loss: 0.1188\n",
      "Epoch 140/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1359 - val_loss: 0.1145\n",
      "Epoch 141/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1359 - val_loss: 0.1171\n",
      "Epoch 142/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1363 - val_loss: 0.1180\n",
      "Epoch 143/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1354 - val_loss: 0.1152\n",
      "Epoch 144/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1361 - val_loss: 0.1166\n",
      "Epoch 145/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1362 - val_loss: 0.1166\n",
      "Epoch 146/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1365 - val_loss: 0.1276\n",
      "Epoch 147/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1363 - val_loss: 0.1200\n",
      "Epoch 148/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1365 - val_loss: 0.1207\n",
      "Epoch 149/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1367 - val_loss: 0.1244\n",
      "Epoch 150/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1364 - val_loss: 0.1198\n",
      "Epoch 151/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1360 - val_loss: 0.1240\n",
      "Epoch 152/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1362 - val_loss: 0.1397\n",
      "Epoch 153/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1370 - val_loss: 0.1258\n",
      "Epoch 154/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1366 - val_loss: 0.1372\n",
      "Epoch 155/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1365 - val_loss: 0.1275\n",
      "Epoch 156/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1365 - val_loss: 0.1415\n",
      "Epoch 157/200\n",
      "1057265/1057265 [==============================] - 68s 64us/sample - loss: 0.1359 - val_loss: 0.1313\n",
      "Epoch 158/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1355 - val_loss: 0.1278\n",
      "Epoch 159/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1351 - val_loss: 0.1291\n",
      "Epoch 160/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1350 - val_loss: 0.1328\n",
      "Epoch 161/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1351 - val_loss: 0.1192\n",
      "Epoch 162/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1347 - val_loss: 0.1206\n",
      "Epoch 163/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1348 - val_loss: 0.1270\n",
      "Epoch 164/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1352 - val_loss: 0.1344\n",
      "Epoch 165/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1353 - val_loss: 0.1335\n",
      "Epoch 166/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1358 - val_loss: 0.1284\n",
      "Epoch 167/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1364 - val_loss: 0.1354\n",
      "Epoch 168/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1359 - val_loss: 0.1243\n",
      "Epoch 169/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1351 - val_loss: 0.1228\n",
      "Epoch 170/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1341 - val_loss: 0.1247\n",
      "Epoch 171/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1343 - val_loss: 0.1159\n",
      "Epoch 172/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1343 - val_loss: 0.1208\n",
      "Epoch 173/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1342 - val_loss: 0.1311\n",
      "Epoch 174/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1337 - val_loss: 0.1199\n",
      "Epoch 175/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1338 - val_loss: 0.1192\n",
      "Epoch 176/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1336 - val_loss: 0.1194\n",
      "Epoch 177/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1335 - val_loss: 0.1187\n",
      "Epoch 178/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1330 - val_loss: 0.1190\n",
      "Epoch 179/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1332 - val_loss: 0.1207\n",
      "Epoch 180/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1332 - val_loss: 0.1255\n",
      "Epoch 181/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1328 - val_loss: 0.1192\n",
      "Epoch 182/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1330 - val_loss: 0.1267\n",
      "Epoch 183/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1330 - val_loss: 0.1250\n",
      "Epoch 184/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1331 - val_loss: 0.1301\n",
      "Epoch 185/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1331 - val_loss: 0.1262\n",
      "Epoch 186/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1332 - val_loss: 0.1249\n",
      "Epoch 187/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1330 - val_loss: 0.1190\n",
      "Epoch 188/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1330 - val_loss: 0.1286\n",
      "Epoch 189/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1335 - val_loss: 0.1224\n",
      "Epoch 190/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1334 - val_loss: 0.1247\n",
      "Epoch 191/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1333 - val_loss: 0.1193\n",
      "Epoch 192/200\n",
      "1057265/1057265 [==============================] - 67s 64us/sample - loss: 0.1330 - val_loss: 0.1264\n",
      "Epoch 193/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1329 - val_loss: 0.1178\n",
      "Epoch 194/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1330 - val_loss: 0.1198\n",
      "Epoch 195/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1324 - val_loss: 0.1237\n",
      "Epoch 196/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1327 - val_loss: 0.1235\n",
      "Epoch 197/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1332 - val_loss: 0.1158\n",
      "Epoch 198/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1340 - val_loss: 0.1243\n",
      "Epoch 199/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1340 - val_loss: 0.1251\n",
      "Epoch 200/200\n",
      "1057265/1057265 [==============================] - 67s 63us/sample - loss: 0.1337 - val_loss: 0.1200\n",
      "Training 590611 samples for 3JHH\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 413427 samples, validate on 177184 samples\n",
      "Epoch 1/200\n",
      "413427/413427 [==============================] - 39s 96us/sample - loss: 0.4373 - val_loss: 0.3542\n",
      "Epoch 2/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.3054 - val_loss: 0.2057\n",
      "Epoch 3/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2691 - val_loss: 0.2856\n",
      "Epoch 4/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2644 - val_loss: 0.2722\n",
      "Epoch 5/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2667 - val_loss: 0.3150\n",
      "Epoch 6/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2547 - val_loss: 0.3362\n",
      "Epoch 7/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2528 - val_loss: 0.2597\n",
      "Epoch 8/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2316 - val_loss: 0.1613\n",
      "Epoch 9/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2208 - val_loss: 0.2180\n",
      "Epoch 10/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2167 - val_loss: 0.2316\n",
      "Epoch 11/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2127 - val_loss: 0.2067\n",
      "Epoch 12/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2109 - val_loss: 0.2115\n",
      "Epoch 13/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2084 - val_loss: 0.2043\n",
      "Epoch 14/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2060 - val_loss: 0.2075\n",
      "Epoch 15/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2051 - val_loss: 0.2292\n",
      "Epoch 16/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2024 - val_loss: 0.2220\n",
      "Epoch 17/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.2006 - val_loss: 0.1907\n",
      "Epoch 18/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1989 - val_loss: 0.1906\n",
      "Epoch 19/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1991 - val_loss: 0.1932\n",
      "Epoch 20/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1969 - val_loss: 0.1880\n",
      "Epoch 21/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1950 - val_loss: 0.1851\n",
      "Epoch 22/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1942 - val_loss: 0.1816\n",
      "Epoch 23/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1936 - val_loss: 0.1624\n",
      "Epoch 24/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1923 - val_loss: 0.1729\n",
      "Epoch 25/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1914 - val_loss: 0.1623\n",
      "Epoch 26/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1903 - val_loss: 0.1591\n",
      "Epoch 27/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1895 - val_loss: 0.1554\n",
      "Epoch 28/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1895 - val_loss: 0.1816\n",
      "Epoch 29/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1883 - val_loss: 0.1593\n",
      "Epoch 30/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1884 - val_loss: 0.1641\n",
      "Epoch 31/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1874 - val_loss: 0.1481\n",
      "Epoch 32/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1870 - val_loss: 0.1540\n",
      "Epoch 33/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1863 - val_loss: 0.1519\n",
      "Epoch 34/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1856 - val_loss: 0.1427\n",
      "Epoch 35/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1854 - val_loss: 0.1436\n",
      "Epoch 36/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1848 - val_loss: 0.1425\n",
      "Epoch 37/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1851 - val_loss: 0.1327\n",
      "Epoch 38/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1848 - val_loss: 0.1231\n",
      "Epoch 39/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1834 - val_loss: 0.1257\n",
      "Epoch 40/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1828 - val_loss: 0.1303\n",
      "Epoch 41/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1826 - val_loss: 0.1186\n",
      "Epoch 42/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1822 - val_loss: 0.1234\n",
      "Epoch 43/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1820 - val_loss: 0.1088\n",
      "Epoch 44/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1816 - val_loss: 0.1333\n",
      "Epoch 45/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1814 - val_loss: 0.1239\n",
      "Epoch 46/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1811 - val_loss: 0.1163\n",
      "Epoch 47/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1805 - val_loss: 0.1180\n",
      "Epoch 48/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1810 - val_loss: 0.1159\n",
      "Epoch 49/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1801 - val_loss: 0.1227\n",
      "Epoch 50/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1802 - val_loss: 0.1199\n",
      "Epoch 51/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1797 - val_loss: 0.1176\n",
      "Epoch 52/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1801 - val_loss: 0.1240\n",
      "Epoch 53/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1798 - val_loss: 0.1194\n",
      "Epoch 54/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1796 - val_loss: 0.1253\n",
      "Epoch 55/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1783 - val_loss: 0.1093\n",
      "Epoch 56/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1783 - val_loss: 0.1156\n",
      "Epoch 57/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1790 - val_loss: 0.1235\n",
      "Epoch 58/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1785 - val_loss: 0.1243\n",
      "Epoch 59/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1783 - val_loss: 0.1173\n",
      "Epoch 60/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1786 - val_loss: 0.1224\n",
      "Epoch 61/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1781 - val_loss: 0.1261\n",
      "Epoch 62/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1780 - val_loss: 0.1173\n",
      "Epoch 63/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1779 - val_loss: 0.1208\n",
      "Epoch 64/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1772 - val_loss: 0.1247\n",
      "Epoch 65/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1775 - val_loss: 0.1297\n",
      "Epoch 66/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1774 - val_loss: 0.1252\n",
      "Epoch 67/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1775 - val_loss: 0.1242\n",
      "Epoch 68/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1769 - val_loss: 0.1195\n",
      "Epoch 69/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1772 - val_loss: 0.1142\n",
      "Epoch 70/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1770 - val_loss: 0.1157\n",
      "Epoch 71/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1778 - val_loss: 0.1127\n",
      "Epoch 72/200\n",
      "413427/413427 [==============================] - 25s 62us/sample - loss: 0.1769 - val_loss: 0.1146\n",
      "Epoch 73/200\n",
      "413427/413427 [==============================] - 27s 65us/sample - loss: 0.1769 - val_loss: 0.1261\n",
      "Epoch 74/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1776 - val_loss: 0.1232\n",
      "Epoch 75/200\n",
      "413427/413427 [==============================] - 26s 62us/sample - loss: 0.1765 - val_loss: 0.1243\n",
      "Epoch 76/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1767 - val_loss: 0.1281\n",
      "Epoch 77/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1768 - val_loss: 0.1221\n",
      "Epoch 78/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1759 - val_loss: 0.1289\n",
      "Epoch 79/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1761 - val_loss: 0.1305\n",
      "Epoch 80/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1760 - val_loss: 0.1303\n",
      "Epoch 81/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1376\n",
      "Epoch 82/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1761 - val_loss: 0.1377\n",
      "Epoch 83/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1759 - val_loss: 0.1276\n",
      "Epoch 84/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1758 - val_loss: 0.1281\n",
      "Epoch 85/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1760 - val_loss: 0.1214\n",
      "Epoch 86/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1200\n",
      "Epoch 87/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1748 - val_loss: 0.1286\n",
      "Epoch 88/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1759 - val_loss: 0.1336\n",
      "Epoch 89/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1768 - val_loss: 0.1317\n",
      "Epoch 90/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1759 - val_loss: 0.1331\n",
      "Epoch 91/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1766 - val_loss: 0.1336\n",
      "Epoch 92/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.1352\n",
      "Epoch 93/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1755 - val_loss: 0.1308\n",
      "Epoch 94/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1760 - val_loss: 0.1290\n",
      "Epoch 95/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1750 - val_loss: 0.1307\n",
      "Epoch 96/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.1335\n",
      "Epoch 97/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1756 - val_loss: 0.1367\n",
      "Epoch 98/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.1310\n",
      "Epoch 99/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1761 - val_loss: 0.1287\n",
      "Epoch 100/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1757 - val_loss: 0.1458\n",
      "Epoch 101/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1758 - val_loss: 0.1250\n",
      "Epoch 102/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1756 - val_loss: 0.1330\n",
      "Epoch 103/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1752 - val_loss: 0.1364\n",
      "Epoch 104/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1756 - val_loss: 0.1466\n",
      "Epoch 105/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.1319\n",
      "Epoch 106/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1757 - val_loss: 0.1276\n",
      "Epoch 107/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1639\n",
      "Epoch 108/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1446\n",
      "Epoch 109/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1753 - val_loss: 0.1453\n",
      "Epoch 110/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1753 - val_loss: 0.1458\n",
      "Epoch 111/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1749 - val_loss: 0.1442\n",
      "Epoch 112/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1759 - val_loss: 0.1419\n",
      "Epoch 113/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1758 - val_loss: 0.1407\n",
      "Epoch 114/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1207\n",
      "Epoch 115/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1752 - val_loss: 0.1218\n",
      "Epoch 116/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1743 - val_loss: 0.1222\n",
      "Epoch 117/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1282\n",
      "Epoch 118/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.1186\n",
      "Epoch 119/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1740 - val_loss: 0.1156\n",
      "Epoch 120/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.1276\n",
      "Epoch 121/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1744 - val_loss: 0.1219\n",
      "Epoch 122/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.1323\n",
      "Epoch 123/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.1384\n",
      "Epoch 124/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.1192\n",
      "Epoch 125/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1433\n",
      "Epoch 126/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1741 - val_loss: 0.1334\n",
      "Epoch 127/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1741 - val_loss: 0.1444\n",
      "Epoch 128/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1744 - val_loss: 0.1259\n",
      "Epoch 129/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1747 - val_loss: 0.1222\n",
      "Epoch 130/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.1508\n",
      "Epoch 131/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.1438\n",
      "Epoch 132/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.1333\n",
      "Epoch 133/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1752 - val_loss: 0.1462\n",
      "Epoch 134/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1747 - val_loss: 0.1305\n",
      "Epoch 135/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.1475\n",
      "Epoch 136/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1756 - val_loss: 0.1456\n",
      "Epoch 137/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1757 - val_loss: 0.1866\n",
      "Epoch 138/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1757 - val_loss: 0.1537\n",
      "Epoch 139/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1758 - val_loss: 0.1756\n",
      "Epoch 140/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1762 - val_loss: 0.1585\n",
      "Epoch 141/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1773 - val_loss: 0.1265\n",
      "Epoch 142/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1769 - val_loss: 0.1096\n",
      "Epoch 143/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1761 - val_loss: 0.1070\n",
      "Epoch 144/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1753 - val_loss: 0.1046\n",
      "Epoch 145/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1757 - val_loss: 0.1050\n",
      "Epoch 146/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.1074\n",
      "Epoch 147/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1756 - val_loss: 0.1025\n",
      "Epoch 148/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1750 - val_loss: 0.1007\n",
      "Epoch 149/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1760 - val_loss: 0.1074\n",
      "Epoch 150/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1751 - val_loss: 0.0989\n",
      "Epoch 151/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1753 - val_loss: 0.1008\n",
      "Epoch 152/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1753 - val_loss: 0.0997\n",
      "Epoch 153/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1751 - val_loss: 0.1000\n",
      "Epoch 154/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1751 - val_loss: 0.1028\n",
      "Epoch 155/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1758 - val_loss: 0.0967\n",
      "Epoch 156/200\n",
      "413427/413427 [==============================] - 26s 64us/sample - loss: 0.1760 - val_loss: 0.0977\n",
      "Epoch 157/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1748 - val_loss: 0.1048\n",
      "Epoch 158/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1747 - val_loss: 0.0979\n",
      "Epoch 159/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.1021\n",
      "Epoch 160/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.0995\n",
      "Epoch 161/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.1010\n",
      "Epoch 162/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1747 - val_loss: 0.1074\n",
      "Epoch 163/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1752 - val_loss: 0.1046\n",
      "Epoch 164/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1740 - val_loss: 0.1003\n",
      "Epoch 165/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.0944\n",
      "Epoch 166/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.0997\n",
      "Epoch 167/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1744 - val_loss: 0.1019\n",
      "Epoch 168/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1749 - val_loss: 0.1024\n",
      "Epoch 169/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1750 - val_loss: 0.0957\n",
      "Epoch 170/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1748 - val_loss: 0.1110\n",
      "Epoch 171/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.1023\n",
      "Epoch 172/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.1019\n",
      "Epoch 173/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1747 - val_loss: 0.0996\n",
      "Epoch 174/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1756 - val_loss: 0.0989\n",
      "Epoch 175/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.0992\n",
      "Epoch 176/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1754 - val_loss: 0.0998\n",
      "Epoch 177/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.0994\n",
      "Epoch 178/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.1024\n",
      "Epoch 179/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1755 - val_loss: 0.0930\n",
      "Epoch 180/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.1116\n",
      "Epoch 181/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1740 - val_loss: 0.1053\n",
      "Epoch 182/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1744 - val_loss: 0.0911\n",
      "Epoch 183/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1748 - val_loss: 0.0981\n",
      "Epoch 184/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1747 - val_loss: 0.0967\n",
      "Epoch 185/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1744 - val_loss: 0.0946\n",
      "Epoch 186/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1744 - val_loss: 0.0988\n",
      "Epoch 187/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1750 - val_loss: 0.1001\n",
      "Epoch 188/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1741 - val_loss: 0.1000\n",
      "Epoch 189/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1746 - val_loss: 0.0982\n",
      "Epoch 190/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1737 - val_loss: 0.1001\n",
      "Epoch 191/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1743 - val_loss: 0.0958\n",
      "Epoch 192/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1745 - val_loss: 0.0976\n",
      "Epoch 193/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1743 - val_loss: 0.0970\n",
      "Epoch 194/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.1020\n",
      "Epoch 195/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1750 - val_loss: 0.0981\n",
      "Epoch 196/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1749 - val_loss: 0.0980\n",
      "Epoch 197/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1744 - val_loss: 0.0968\n",
      "Epoch 198/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.0928\n",
      "Epoch 199/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1742 - val_loss: 0.0931\n",
      "Epoch 200/200\n",
      "413427/413427 [==============================] - 26s 63us/sample - loss: 0.1738 - val_loss: 0.1097\n",
      "Training 166415 samples for 3JHN\n",
      "  Setting up data\n",
      "  Creating model\n",
      "  Fitting model\n",
      "Train on 116490 samples, validate on 49925 samples\n",
      "Epoch 1/200\n",
      "116490/116490 [==============================] - 21s 180us/sample - loss: 0.7763 - val_loss: 1.3851\n",
      "Epoch 2/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.5399 - val_loss: 1.2829\n",
      "Epoch 3/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.4677 - val_loss: 0.8566\n",
      "Epoch 4/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.4312 - val_loss: 0.6071\n",
      "Epoch 5/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.4063 - val_loss: 0.7247\n",
      "Epoch 6/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3903 - val_loss: 1.3596\n",
      "Epoch 7/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.3794 - val_loss: 0.6464\n",
      "Epoch 8/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.3621 - val_loss: 1.4601\n",
      "Epoch 9/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.3562 - val_loss: 0.4588\n",
      "Epoch 10/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3457 - val_loss: 0.8981\n",
      "Epoch 11/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3402 - val_loss: 2.3047\n",
      "Epoch 12/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3314 - val_loss: 4.4832\n",
      "Epoch 13/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.3213 - val_loss: 0.7369\n",
      "Epoch 14/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3185 - val_loss: 0.8332\n",
      "Epoch 15/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3138 - val_loss: 0.8950\n",
      "Epoch 16/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.3092 - val_loss: 1.3368\n",
      "Epoch 17/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.3022 - val_loss: 2.4276\n",
      "Epoch 18/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2993 - val_loss: 5.7181\n",
      "Epoch 19/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2964 - val_loss: 8.9348\n",
      "Epoch 20/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2919 - val_loss: 2.6406\n",
      "Epoch 21/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2881 - val_loss: 1.0233\n",
      "Epoch 22/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2843 - val_loss: 4.4560\n",
      "Epoch 23/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2871 - val_loss: 3.5837\n",
      "Epoch 24/200\n",
      "116490/116490 [==============================] - 7s 61us/sample - loss: 0.2835 - val_loss: 0.9885\n",
      "Epoch 25/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2762 - val_loss: 4.6203\n",
      "Epoch 26/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2856 - val_loss: 0.3807\n",
      "Epoch 27/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2757 - val_loss: 0.9981\n",
      "Epoch 28/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2705 - val_loss: 1.0695\n",
      "Epoch 29/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2730 - val_loss: 3.1636\n",
      "Epoch 30/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2705 - val_loss: 4.3683\n",
      "Epoch 31/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2704 - val_loss: 2.9404\n",
      "Epoch 32/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2690 - val_loss: 0.3550\n",
      "Epoch 33/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2692 - val_loss: 1.4736\n",
      "Epoch 34/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2708 - val_loss: 0.3684\n",
      "Epoch 35/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2653 - val_loss: 0.4816\n",
      "Epoch 36/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2618 - val_loss: 3.7581\n",
      "Epoch 37/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2686 - val_loss: 0.3862\n",
      "Epoch 38/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2635 - val_loss: 0.5373\n",
      "Epoch 39/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2586 - val_loss: 0.3740\n",
      "Epoch 40/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2520 - val_loss: 0.4057\n",
      "Epoch 41/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2477 - val_loss: 0.3699\n",
      "Epoch 42/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2435 - val_loss: 0.3118\n",
      "Epoch 43/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2407 - val_loss: 0.3834\n",
      "Epoch 44/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2400 - val_loss: 0.3661\n",
      "Epoch 45/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2378 - val_loss: 2.0234\n",
      "Epoch 46/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2367 - val_loss: 0.3468\n",
      "Epoch 47/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2348 - val_loss: 0.3549\n",
      "Epoch 48/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2336 - val_loss: 0.3731\n",
      "Epoch 49/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2305 - val_loss: 1.5448\n",
      "Epoch 50/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2301 - val_loss: 0.3600\n",
      "Epoch 51/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2278 - val_loss: 0.3198\n",
      "Epoch 52/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2263 - val_loss: 0.3448\n",
      "Epoch 53/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2265 - val_loss: 0.3534\n",
      "Epoch 54/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2250 - val_loss: 0.3479\n",
      "Epoch 55/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2245 - val_loss: 0.3356\n",
      "Epoch 56/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2210 - val_loss: 0.3394\n",
      "Epoch 57/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2212 - val_loss: 0.3468\n",
      "Epoch 58/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2218 - val_loss: 0.4084\n",
      "Epoch 59/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2182 - val_loss: 0.3601\n",
      "Epoch 60/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2196 - val_loss: 0.3416\n",
      "Epoch 61/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2173 - val_loss: 0.2901\n",
      "Epoch 62/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2156 - val_loss: 0.3529\n",
      "Epoch 63/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2148 - val_loss: 0.3349\n",
      "Epoch 64/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2151 - val_loss: 0.3387\n",
      "Epoch 65/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2116 - val_loss: 0.3347\n",
      "Epoch 66/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2122 - val_loss: 0.3547\n",
      "Epoch 67/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2145 - val_loss: 0.3756\n",
      "Epoch 68/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2121 - val_loss: 0.3320\n",
      "Epoch 69/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2132 - val_loss: 0.3576\n",
      "Epoch 70/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2110 - val_loss: 0.3307\n",
      "Epoch 71/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2094 - val_loss: 0.3210\n",
      "Epoch 72/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2087 - val_loss: 0.3296\n",
      "Epoch 73/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2076 - val_loss: 0.3279\n",
      "Epoch 74/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2054 - val_loss: 0.3258\n",
      "Epoch 75/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2053 - val_loss: 0.3191\n",
      "Epoch 76/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2035 - val_loss: 0.3283\n",
      "Epoch 77/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2050 - val_loss: 0.3445\n",
      "Epoch 78/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2035 - val_loss: 0.3285\n",
      "Epoch 79/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2037 - val_loss: 0.3260\n",
      "Epoch 80/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2029 - val_loss: 0.2730\n",
      "Epoch 81/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.2011 - val_loss: 0.3207\n",
      "Epoch 82/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2012 - val_loss: 0.3248\n",
      "Epoch 83/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2020 - val_loss: 0.3208\n",
      "Epoch 84/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.2032 - val_loss: 0.3198\n",
      "Epoch 85/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1995 - val_loss: 0.2790\n",
      "Epoch 86/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1996 - val_loss: 0.2914\n",
      "Epoch 87/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1989 - val_loss: 0.3728\n",
      "Epoch 88/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1984 - val_loss: 0.2758\n",
      "Epoch 89/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1975 - val_loss: 0.2788\n",
      "Epoch 90/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1966 - val_loss: 0.2924\n",
      "Epoch 91/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1965 - val_loss: 0.3178\n",
      "Epoch 92/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1949 - val_loss: 0.3199\n",
      "Epoch 93/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1956 - val_loss: 0.3154\n",
      "Epoch 94/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1947 - val_loss: 0.3090\n",
      "Epoch 95/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1953 - val_loss: 0.3087\n",
      "Epoch 96/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1945 - val_loss: 0.2627\n",
      "Epoch 97/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1933 - val_loss: 0.3187\n",
      "Epoch 98/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1920 - val_loss: 0.3277\n",
      "Epoch 99/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1911 - val_loss: 0.3222\n",
      "Epoch 100/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1897 - val_loss: 0.2569\n",
      "Epoch 101/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1893 - val_loss: 0.3308\n",
      "Epoch 102/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1898 - val_loss: 0.3360\n",
      "Epoch 103/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1902 - val_loss: 0.3092\n",
      "Epoch 104/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1895 - val_loss: 0.3047\n",
      "Epoch 105/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1882 - val_loss: 0.2968\n",
      "Epoch 106/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1869 - val_loss: 0.3181\n",
      "Epoch 107/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1871 - val_loss: 0.2959\n",
      "Epoch 108/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1868 - val_loss: 0.2856\n",
      "Epoch 109/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1853 - val_loss: 0.3114\n",
      "Epoch 110/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1858 - val_loss: 0.3116\n",
      "Epoch 111/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1854 - val_loss: 0.3129\n",
      "Epoch 112/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1851 - val_loss: 0.2892\n",
      "Epoch 113/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1851 - val_loss: 0.3037\n",
      "Epoch 114/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1852 - val_loss: 0.3040\n",
      "Epoch 115/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1844 - val_loss: 0.3075\n",
      "Epoch 116/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1866 - val_loss: 0.3166\n",
      "Epoch 117/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1845 - val_loss: 0.3029\n",
      "Epoch 118/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1834 - val_loss: 0.2569\n",
      "Epoch 119/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1840 - val_loss: 0.3161\n",
      "Epoch 120/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1819 - val_loss: 0.3152\n",
      "Epoch 121/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1821 - val_loss: 0.3132\n",
      "Epoch 122/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1819 - val_loss: 0.2847\n",
      "Epoch 123/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1820 - val_loss: 0.2657\n",
      "Epoch 124/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1818 - val_loss: 0.3064\n",
      "Epoch 125/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1805 - val_loss: 0.2741\n",
      "Epoch 126/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1808 - val_loss: 0.2861\n",
      "Epoch 127/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1801 - val_loss: 0.2941\n",
      "Epoch 128/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1793 - val_loss: 0.2825\n",
      "Epoch 129/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1790 - val_loss: 0.2641\n",
      "Epoch 130/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1787 - val_loss: 0.2954\n",
      "Epoch 131/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1789 - val_loss: 0.3139\n",
      "Epoch 132/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1791 - val_loss: 0.2692\n",
      "Epoch 133/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1791 - val_loss: 0.2650\n",
      "Epoch 134/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1775 - val_loss: 0.2971\n",
      "Epoch 135/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1773 - val_loss: 0.3171\n",
      "Epoch 136/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1795 - val_loss: 0.3050\n",
      "Epoch 137/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1774 - val_loss: 0.2847\n",
      "Epoch 138/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1760 - val_loss: 0.2803\n",
      "Epoch 139/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1755 - val_loss: 0.2890\n",
      "Epoch 140/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1774 - val_loss: 0.3021\n",
      "Epoch 141/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1761 - val_loss: 0.2998\n",
      "Epoch 142/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1757 - val_loss: 0.2913\n",
      "Epoch 143/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1767 - val_loss: 0.2894\n",
      "Epoch 144/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1755 - val_loss: 0.2936\n",
      "Epoch 145/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1760 - val_loss: 0.2939\n",
      "Epoch 146/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1748 - val_loss: 0.3776\n",
      "Epoch 147/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1745 - val_loss: 0.2830\n",
      "Epoch 148/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1737 - val_loss: 0.2706\n",
      "Epoch 149/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1745 - val_loss: 0.2692\n",
      "Epoch 150/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1745 - val_loss: 0.2923\n",
      "Epoch 151/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1732 - val_loss: 0.3027\n",
      "Epoch 152/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1731 - val_loss: 0.2986\n",
      "Epoch 153/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1741 - val_loss: 0.3183\n",
      "Epoch 154/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1739 - val_loss: 0.3634\n",
      "Epoch 155/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1742 - val_loss: 0.3647\n",
      "Epoch 156/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1734 - val_loss: 0.2519\n",
      "Epoch 157/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1726 - val_loss: 0.2937\n",
      "Epoch 158/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1724 - val_loss: 0.2880\n",
      "Epoch 159/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1723 - val_loss: 0.2615\n",
      "Epoch 160/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1730 - val_loss: 0.2998\n",
      "Epoch 161/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1740 - val_loss: 0.2735\n",
      "Epoch 162/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1725 - val_loss: 0.2845\n",
      "Epoch 163/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1715 - val_loss: 0.5235\n",
      "Epoch 164/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1717 - val_loss: 0.2576\n",
      "Epoch 165/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1701 - val_loss: 0.2931\n",
      "Epoch 166/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1710 - val_loss: 0.2763\n",
      "Epoch 167/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1726 - val_loss: 0.2770\n",
      "Epoch 168/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1714 - val_loss: 0.2637\n",
      "Epoch 169/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1717 - val_loss: 0.2732\n",
      "Epoch 170/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1700 - val_loss: 0.3371\n",
      "Epoch 171/200\n",
      "116490/116490 [==============================] - 7s 61us/sample - loss: 0.1693 - val_loss: 0.3398\n",
      "Epoch 172/200\n",
      "116490/116490 [==============================] - 7s 61us/sample - loss: 0.1706 - val_loss: 0.2881\n",
      "Epoch 173/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1707 - val_loss: 0.3551\n",
      "Epoch 174/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1701 - val_loss: 0.3159\n",
      "Epoch 175/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1697 - val_loss: 0.2941\n",
      "Epoch 176/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1700 - val_loss: 0.3059\n",
      "Epoch 177/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1683 - val_loss: 0.2833\n",
      "Epoch 178/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1688 - val_loss: 0.3175\n",
      "Epoch 179/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1720 - val_loss: 0.2746\n",
      "Epoch 180/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1698 - val_loss: 0.2858\n",
      "Epoch 181/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1710 - val_loss: 0.2984\n",
      "Epoch 182/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1710 - val_loss: 0.2680\n",
      "Epoch 183/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1683 - val_loss: 0.2581\n",
      "Epoch 184/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1679 - val_loss: 0.2990\n",
      "Epoch 185/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1680 - val_loss: 0.3078\n",
      "Epoch 186/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1695 - val_loss: 0.3875\n",
      "Epoch 187/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1691 - val_loss: 0.2729\n",
      "Epoch 188/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1681 - val_loss: 0.2758\n",
      "Epoch 189/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1680 - val_loss: 0.2895\n",
      "Epoch 190/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1687 - val_loss: 0.2883\n",
      "Epoch 191/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1689 - val_loss: 0.2954\n",
      "Epoch 192/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1687 - val_loss: 0.2968\n",
      "Epoch 193/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1671 - val_loss: 0.2833\n",
      "Epoch 194/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1673 - val_loss: 0.2823\n",
      "Epoch 195/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1668 - val_loss: 0.2771\n",
      "Epoch 196/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1663 - val_loss: 0.2817\n",
      "Epoch 197/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1669 - val_loss: 0.2739\n",
      "Epoch 198/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1671 - val_loss: 0.2373\n",
      "Epoch 199/200\n",
      "116490/116490 [==============================] - 7s 62us/sample - loss: 0.1657 - val_loss: 0.2836\n",
      "Epoch 200/200\n",
      "116490/116490 [==============================] - 7s 63us/sample - loss: 0.1654 - val_loss: 0.2636\n"
     ]
    }
   ],
   "source": [
    "from models import partition_data, NNModel\n",
    "\n",
    "def train(data):\n",
    "    models = {}\n",
    "    for t in sorted(data.type.unique()):\n",
    "        train_df = data[data.type == t]\n",
    "        print(f'Training {len(train_df)} samples for {t}')\n",
    "        model = NNModel(dict(molecules=molecules, \n",
    "                             structures=structures),\n",
    "                        **nn_args)\n",
    "        models[t] = model\n",
    "        model.fit(train_df, train_df)\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = train(labelled_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2505542, 14)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_enh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying\n"
     ]
    }
   ],
   "source": [
    "from models import NNModel\n",
    "\n",
    "print('Copying')\n",
    "new_models = {}\n",
    "for name in models:\n",
    "    new_models[name] = NNModel(dict(molecules=molecules, \n",
    "                                    structures=structures),\n",
    "                               **nn_args)\n",
    "    new_models[name].model = models[name].model\n",
    "    new_models[name].input_scaler = models[name].input_scaler\n",
    "    new_models[name].output_scaler = models[name].output_scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 380609 samples for 1JHC\n",
      "  Setting up data\n",
      "  Predicting\n",
      "Predicting 24195 samples for 1JHN\n",
      "  Setting up data\n",
      "  Predicting\n",
      "Predicting 613138 samples for 2JHC\n",
      "  Setting up data\n",
      "[13, 4, 3, 0, 1, 8] 13 8 \n",
      " dsgdb9nsd_037497\n",
      "[17, 8, 1, 0, 3, 4] 17 4 \n",
      " dsgdb9nsd_037497\n",
      "[12, 6, 7, 1] 12 1 \n",
      " dsgdb9nsd_072320\n",
      "[14, 3, 2, 1, 8] 14 8 \n",
      " dsgdb9nsd_133863\n",
      "[20, 7, 0, 1, 8] 20 8 \n",
      " dsgdb9nsd_133863\n",
      "  Predicting\n",
      "Predicting 203126 samples for 2JHH\n",
      "  Setting up data\n",
      "  Predicting\n",
      "Predicting 64424 samples for 2JHN\n",
      "  Setting up data\n",
      "  Predicting\n",
      "Predicting 811999 samples for 3JHC\n",
      "  Setting up data\n",
      "[11, 1, 0, 3, 4] 11 4 \n",
      " dsgdb9nsd_037497\n",
      "[13, 4, 3, 0, 1] 13 1 \n",
      " dsgdb9nsd_037497\n",
      "[13, 4, 5, 6, 7] 13 7 \n",
      " dsgdb9nsd_037497\n",
      "[14, 5, 6, 7, 8] 14 8 \n",
      " dsgdb9nsd_037497\n",
      "[15, 5, 6, 7, 8] 15 8 \n",
      " dsgdb9nsd_037497\n",
      "[16, 7, 6, 5, 4] 16 4 \n",
      " dsgdb9nsd_037497\n",
      "[17, 8, 7, 6, 5] 17 5 \n",
      " dsgdb9nsd_037497\n",
      "[9, 0, 1, 7, 6] 9 6 \n",
      " dsgdb9nsd_072320\n",
      "[10, 2, 1, 7, 6] 10 6 \n",
      " dsgdb9nsd_072320\n",
      "[11, 2, 1, 7, 6] 11 6 \n",
      " dsgdb9nsd_072320\n",
      "[12, 6, 7, 1, 2] 12 2 \n",
      " dsgdb9nsd_072320\n",
      "[14, 3, 2, 1, 0, 7] 14 7 \n",
      " dsgdb9nsd_133863\n",
      "[20, 7, 0, 1, 2, 3] 20 3 \n",
      " dsgdb9nsd_133863\n",
      "  Predicting\n",
      "Predicting 317435 samples for 3JHH\n",
      "  Setting up data\n",
      "[13, 4, 3, 0, 1, 8, 17] 13 17 \n",
      " dsgdb9nsd_037497\n",
      "  Predicting\n",
      "Predicting 90616 samples for 3JHN\n",
      "  Setting up data\n",
      "[17, 8, 1, 0, 3] 17 3 \n",
      " dsgdb9nsd_037497\n",
      "  Predicting\n"
     ]
    }
   ],
   "source": [
    "from models import NNModel\n",
    "\n",
    "def predict(data, models):\n",
    "    out_df = None\n",
    "    \n",
    "    for t in sorted(data.type.unique()):\n",
    "        predict_df = data[data.type == t]\n",
    "        print(f'Predicting {len(predict_df)} samples for {t}')\n",
    "        output = models[t].predict(predict_df)\n",
    "        \n",
    "        id = predict_df['id']\n",
    "        out_df_coupling = pd.DataFrame(data={'id':id, 'scalar_coupling_constant':output.flatten()}, index=predict_df.index)\n",
    "        \n",
    "        if out_df is None:\n",
    "            out_df = out_df_coupling\n",
    "        else:\n",
    "            out_df = out_df.append(out_df_coupling).sort_index()\n",
    "\n",
    "    return out_df.sort_index()\n",
    "    \n",
    "#%prun -s cumulative f(unlabelled.head(10000))\n",
    "prediction = predict(unlabelled_enh, new_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>16.242338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>199.019104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>3.099208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>199.123688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>16.252218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147                 16.242338\n",
       "1  4658148                199.019104\n",
       "2  4658149                  3.099208\n",
       "3  4658150                199.123688\n",
       "4  4658151                 16.252218"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv('../data/pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nn.pickle', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_strptime_time() missing 1 required positional argument: 'data_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5ed16b0414d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _strptime_time() missing 1 required positional argument: 'data_string'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.strptime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
