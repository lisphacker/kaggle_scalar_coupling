{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from chemistry import Molecule\n",
    "from util import score\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/molecules.pickle', 'rb') as f:\n",
    "    molecules = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = pd.read_feather('../data/structures.feather')\n",
    "labelled = pd.read_feather('../data/train.feather')\n",
    "unlabelled = pd.read_feather('../data/test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: dsgdb9nsd_000001\n",
       "Atoms:\n",
       "  C 0: [-0.01269814  1.0858041   0.008001  ]\n",
       "  H 1: [ 0.00215042 -0.00603132  0.00197612]\n",
       "  H 2: [1.0117308e+00 1.4637512e+00 2.7657481e-04]\n",
       "  H 3: [-0.54081506  1.4475266  -0.8766437 ]\n",
       "  H 4: [-0.5238136  1.4379326  0.9063973]\n",
       "Bonds:\n",
       "  C(0) - H(1)\n",
       "  C(0) - H(2)\n",
       "  C(0) - H(3)\n",
       "  C(0) - H(4)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecules['dsgdb9nsd_000001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): Bond(dist=1.0919529, valency=1, strength=411),\n",
       " (0, 2): Bond(dist=1.0919516, valency=1, strength=411),\n",
       " (0, 3): Bond(dist=1.0919464, valency=1, strength=411),\n",
       " (0, 4): Bond(dist=1.0919476, valency=1, strength=411)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecules['dsgdb9nsd_000001'].bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "0   0  dsgdb9nsd_000001             1             0  1JHC   \n",
       "1   1  dsgdb9nsd_000001             1             2  2JHH   \n",
       "2   2  dsgdb9nsd_000001             1             3  2JHH   \n",
       "3   3  dsgdb9nsd_000001             1             4  2JHH   \n",
       "4   4  dsgdb9nsd_000001             2             0  1JHC   \n",
       "5   5  dsgdb9nsd_000001             2             3  2JHH   \n",
       "6   6  dsgdb9nsd_000001             2             4  2JHH   \n",
       "7   7  dsgdb9nsd_000001             3             0  1JHC   \n",
       "8   8  dsgdb9nsd_000001             3             4  2JHH   \n",
       "9   9  dsgdb9nsd_000001             4             0  1JHC   \n",
       "\n",
       "   scalar_coupling_constant  \n",
       "0                 84.807602  \n",
       "1                -11.257000  \n",
       "2                -11.254800  \n",
       "3                -11.254300  \n",
       "4                 84.807404  \n",
       "5                -11.254100  \n",
       "6                -11.254800  \n",
       "7                 84.809303  \n",
       "8                -11.254300  \n",
       "9                 84.809502  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "* bond type\n",
    "  * One hot encoding\n",
    "* bond length\n",
    "* atom1 position\n",
    "* atom2 position\n",
    "* atom3 position\n",
    "* atom4 position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_coeff = labelled.scalar_coupling_constant.min()\n",
    "max_coeff = labelled.scalar_coupling_constant.max()\n",
    "\n",
    "offset = min_coeff\n",
    "scale = max_coeff - min_coeff\n",
    "\n",
    "labelled['norm_scc'] = (labelled.scalar_coupling_constant - min_coeff) / (max_coeff - min_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = labelled.type.unique()\n",
    "atoms = structures.atom.unique()\n",
    "\n",
    "index = 0\n",
    "type_index = {}\n",
    "for t in types:\n",
    "    type_index[t] = index\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "atom_index = {}\n",
    "for a in atoms:\n",
    "    atom_index[a] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_labelled(data, count=None, train_frac=0.7):\n",
    "    n_labelled = count if count is not None else len(labelled)\n",
    "    n_train = int(n_labelled * train_frac)\n",
    "    n_test = n_labelled - n_train\n",
    "    indices = np.arange(0, n_labelled)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[0:n_train]\n",
    "    test_indices = indices[n_train:]\n",
    "    \n",
    "    train = data.iloc[train_indices, :]\n",
    "    test = data.iloc[test_indices, :]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def make_input(data):\n",
    "    n = len(data)\n",
    "    \n",
    "\n",
    "    coupling_input = np.zeros((len(types), n), dtype='float32')\n",
    "    atom_input = [None] * 4\n",
    "    for i in range(len(atom_input)):\n",
    "        atom_input[i] = np.zeros((len(atoms), n), dtype='float32')\n",
    "\n",
    "    bond_input = [None] * 3\n",
    "    for i in range(len(bond_input)):\n",
    "        bond_input[i] = np.zeros((3, n), dtype='float32')\n",
    "\n",
    "    for t in type_index:\n",
    "        coupling_input[type_index[t], data.type == t] = 1\n",
    "\n",
    "    for i, row in enumerate(data.itertuples()):\n",
    "        # coupling_input[type_index[row.type], i] = 1\n",
    "        \n",
    "        m = molecules[row.molecule_name]\n",
    "        bonds = m.bonds\n",
    "        \n",
    "        path = m.compute_path(row.atom_index_0, row.atom_index_1)\n",
    "        syms = [m.symbols[idx] for idx in path]\n",
    "        \n",
    "        atom_input[0][atom_index[syms[0]], i]  = 1\n",
    "        \n",
    "        try:\n",
    "            i0 = path[0]\n",
    "            for j, i1 in enumerate(path[1:]):\n",
    "                b = bonds.get((i0, i1), None)\n",
    "                if b is None:\n",
    "                    b = bonds.get((i1, i0), None)\n",
    "                if b is None:\n",
    "                    print(f'Unable to resolve bond - path = {path}, bond = {(i0, i1)})')\n",
    "                    i0 = i1\n",
    "                    continue\n",
    "\n",
    "                j2 = j + 1\n",
    "\n",
    "                bond_input[j][:, i] = [b.dist, b.valency, b.strength]            \n",
    "                atom_input[j2][atom_index[syms[j2]], i] = 1\n",
    "\n",
    "                i0 = i1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return (coupling_input, atom_input, bond_input)\n",
    "\n",
    "def make_output(data):\n",
    "    n = len(data)\n",
    "    output = np.zeros(n, dtype='float32')\n",
    "    #output[:] = data.norm_scc\n",
    "    output[:] = data.scalar_coupling_constant\n",
    "        \n",
    "    return output\n",
    "\n",
    "def combine_inputs(inputs_tuple):\n",
    "    coupling_input, atom_input, bond_input = inputs_tuple\n",
    "    \n",
    "    inputs = [coupling_input]\n",
    "    inputs.extend(atom_input)\n",
    "    inputs.extend(bond_input)\n",
    "    \n",
    "    w, h = coupling_input.shape\n",
    "    for a in atom_input:\n",
    "        w += a.shape[0]\n",
    "    for a in bond_input:\n",
    "        w += a.shape[0]\n",
    "    \n",
    "    input = np.empty((w, h), dtype='float32')\n",
    "    i = 0\n",
    "    for a in inputs:\n",
    "        w, _ = a.shape\n",
    "        input[i:i + w, :] = a\n",
    "        i += w\n",
    "    \n",
    "    return input.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1JHC, 2JHH, 1JHN, 2JHN, 2JHC, 3JHH, 3JHC, 3JHN]\n",
       "Categories (8, object): [1JHC, 2JHH, 1JHN, 2JHN, 2JHC, 3JHH, 3JHC, 3JHN]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_model(inputs):\n",
    "    coupling_input, atom_inputs, bond_inputs = inputs\n",
    "    \n",
    "    n_bond_features, n_samples  = bond_inputs[0].shape\n",
    "    n_atom_types = len(atoms)\n",
    "    n_cpl_types = len(types)\n",
    "    \n",
    "    cpl_ip = Input(shape=(n_cpl_types, ), dtype='float32')\n",
    "    \n",
    "    for i in range(len(atom_inputs)):\n",
    "        atom_ip = Input(shape=(n_atom_types, ), dtype='float32')\n",
    "    \n",
    "    bond_scalers = []\n",
    "    for i in range(len(bond_inputs)):\n",
    "        atom_ip = Input(shape=(n_bond_features, ), dtype='float32')\n",
    "        bond_scalers.push(StandardScaler())\n",
    "    \n",
    "    \n",
    "create_model(make_input(labelled.head(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR,LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "models = {}\n",
    "types = labelled.type.unique()\n",
    "\n",
    "def test(count=1000, max_iter=1000):\n",
    "    for t in types:\n",
    "        labelled_train, labelled_test = partition_labelled(labelled[labelled.type == t], count)\n",
    "\n",
    "        labelled_train_input = combine_inputs(make_input(labelled_train))\n",
    "        labelled_train_output = make_output(labelled_train)\n",
    "\n",
    "        labelled_test_input = combine_inputs(make_input(labelled_test))\n",
    "        labelled_test_output = make_output(labelled_test)\n",
    "\n",
    "        n_train_samples, n_features = labelled_train_input.shape\n",
    "        n_test_samples, _ = labelled_test_input.shape\n",
    "\n",
    "        #input_scaler = StandardScaler()\n",
    "        #output_scaler = StandardScaler()\n",
    "\n",
    "        #reg = LinearSVR(max_iter=max_iter)\n",
    "        reg = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "        #model = Pipeline(steps=[('scaler', input_scaler), ('reg', reg)])\n",
    "        model = Pipeline(steps=[('reg', reg)])\n",
    "\n",
    "        #output_scaler.fit(labelled_train_output.reshape((n_train_samples, 1)))\n",
    "        #scaled_train_output = output_scaler.transform(labelled_train_output.reshape((n_train_samples, 1))).flatten()\n",
    "        scaled_train_output = labelled_train_output\n",
    "\n",
    "        model.fit(labelled_train_input, scaled_train_output)\n",
    "\n",
    "        #s = model.score(input_scaler.transform(labelled_test_input), output_scaler.transform(labelled_test_output.reshape((n_test_samples, 1))).flatten())\n",
    "        scaled_output = model.predict(labelled_test_input)\n",
    "        #output = output_scaler.inverse_transform(scaled_output)\n",
    "        output = scaled_output\n",
    "\n",
    "        s = score(labelled_test, labelled_test_output, output)\n",
    "        \n",
    "        #pprint(list(zip(labelled_test_output, output)))\n",
    "        print(t, s)\n",
    "    \n",
    "test(1000, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labelled_train, labelled_test = partition_labelled(labelled, 20000)\n",
    "\n",
    "labelled_train_input = combine_inputs(make_input(labelled_train))\n",
    "labelled_train_output = make_output(labelled_train)\n",
    "\n",
    "labelled_test_input = combine_inputs(make_input(labelled_test))\n",
    "labelled_test_output = make_output(labelled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVR,LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = LinearSVR()\n",
    "model.fit(labelled_train_input, labelled_train_output)\n",
    "print('')\n",
    "#print(model.predict(test_input))\n",
    "#print(test_output)\n",
    "model.score(labelled_test_input, labelled_test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_train_input = combine_inputs(make_input(labelled.head(10000)))\n",
    "labelled_train_output = make_output(labelled.head(10000))\n",
    "\n",
    "labelled_train_input.shape, labelled_train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_models(df, max_iter=1000):\n",
    "    models = {}\n",
    "    for t in types:\n",
    "        df_coupling = df[df.type == t]\n",
    "        print(f'Training {len(df_coupling)} samples for coupling type {t}')\n",
    "\n",
    "        input = combine_inputs(make_input(df_coupling))\n",
    "        output = make_output(df_coupling)\n",
    "\n",
    "        n_samples, n_features = input.shape\n",
    "\n",
    "        input_scaler = StandardScaler()\n",
    "        output_scaler = StandardScaler()\n",
    "\n",
    "        reg = LinearSVR(max_iter=max_iter)\n",
    "\n",
    "        model = Pipeline(steps=[('scaler', input_scaler), ('reg', reg)])\n",
    "\n",
    "        output_scaler.fit(output.reshape((n_samples, 1)))\n",
    "      \n",
    "        scaled_output = output_scaler.transform(output.reshape((n_samples, 1))).flatten()\n",
    "        model.fit(input, scaled_output)\n",
    "        \n",
    "        models[t] = model\n",
    "        \n",
    "    return models, output_scaler\n",
    "\n",
    "models, output_scaler = create_and_train_models(labelled, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(df, models, output_scaler):\n",
    "    out_df = None\n",
    "    for t in types:\n",
    "        df_coupling = df[df.type == t]\n",
    "        print(f'Predicting {len(df_coupling)} samples for coupling type {t}')\n",
    "\n",
    "        input = combine_inputs(make_input(df_coupling))\n",
    "        n_samples, n_features = input.shape\n",
    "    \n",
    "        if n_samples == 0:\n",
    "            continue\n",
    "\n",
    "        id = df_coupling['id']\n",
    "        scaled_output = models[t].predict(input)\n",
    "        output = output_scaler.inverse_transform(scaled_output.reshape(n_samples, 1)).flatten()\n",
    "        \n",
    "        out_df_coupling = pd.DataFrame(data={'id':id, 'scalar_coupling_constant':output}, index=df_coupling.index)\n",
    "        \n",
    "        if out_df is None:\n",
    "            out_df = out_df_coupling\n",
    "        else:\n",
    "            out_df = out_df.append(out_df_coupling).sort_index()\n",
    "\n",
    "    return out_df.sort_index()\n",
    "                \n",
    "output_df = run_models(unlabelled, models, output_scaler)\n",
    "#output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlabelled_output = model.predict(unlabelled_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlabelled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlabelled_output = unlabelled_output * scale + offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlabelled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_df = pd.DataFrame({'id':unlabelled.id, 'scalar_coupling_constant':pd.Series(unlabelled_output, index=unlabelled.index)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('../data/pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
